---
title: "Round 1 Sandbox and Notes for Updates"
author: "Dan Johnson"
date: "11/28/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Progress Report

*	All raw work?
*	Notes and scribbles in .rmd
*	Train of thought journals?

4.  I have specific R coding questions in Round_1_Data_intake_method.Rmd.


## R Sandbox

Development plan should be to build the workflow using Jerid's "pipeline" template. 

Model for a pipeline is to have a standard file name format like:

Pipeline_NameOfProcess_ver#.r

It calls scripts directly using ```source("xxx.r")``` lines.  

This resembles how I am working already, building individual steps as blocks that can be assembled and organized many ways.

Think of the documentation of a pipeline as saying where each block begins and ends. What are its inputs, outputs?


Build with source(".") files
  Each calls a piece of a larger process.
  
Keeping Confidentiality  
  Use .gitignore to set what does NOT get moved to GitHub
    https://help.github.com/articles/ignoring-files/
    https://git-scm.com/docs/gitignore



##Using Rio for data input

  https://rdrr.io/cran/rio/man/import.html

  https://riptutorial.com/r/example/1580/importing-data-with-rio

  https://github.com/leeper/rio


##Tools

http://www.unit-conversion.info/texttools/random-string-generator/

A random alphanumeric generator. Took 3 seconds to generate 10,000 non-repeating 8-character strings to use for random IDs.


##Modifications to Data Tables Names, Added Data Files

**scrap_names.csv** is a dummy file for testing join methods

**TA_and_Student_IDs.csv** is the file that maps student emails and TA names to their unique ID numbers. If I can keep it behind privacy wall, the dataset will be de-identifiable using ID values only.

In main dataset:
```
Added "unique.record". Set each semester, adds unique ID for each comment record. Format is Sp18.00001, Sp18.00002,...

  Modified column names:
    report.id
						sort (can drop)
    report.title
						student (Still need to de-identify)
    course
						ta (Still need to de-identify)
    lab
						tag (Student enters, but is potentially incorrect)
    type.TA
    grade.TA
    grading.time
						Rank (can drop)

    hypothesis.ok
    data.ok
    citation.ok
    interpretation.ok
    organization.ok
    techflaws.ok
    writing.ok
    comments.incorporated

    ta.comment

    code.subject
    code.structure
    code.locus
    code.scope
    code.tone
    code.notes
```




http://www.sthda.com/english/wiki/tidyr-crucial-step-reshaping-data-with-r-for-easier-analyses

gather()
Collapse multiple columns into key-value pairs. Collapse columns into rows.

spread()
Separate a set of key-value pairs into multiple columns. Spread rows into columns.



separate()
Separate one column into several at a defined character

unite()
Unite multiple columns into one using some kind of character to connect. Think connecting words in columns using "\_" to write snake\_case.


https://r4ds.had.co.nz/tidy-data.html



gather(data, key, value, ...)
	Data is the data frame
	Key, value are names of the two columns being used to create output.
	... specifies columns to gather. 
	
```
my_data2 <- gather(my_data,
                   key = "arrest_attribute",
                   value = "arrest_estimate",
                   -state)
my_data2
```
Sorry but key-value pairs is not making sense to me. 
	Is it that k/v pairs are not the strict two-part data structures I am thinking about from RDS, and more a way to think bout the data as it moves? Is the alternative way to think about it that first column is coming from the table header (key) and appropriate value for that row comes along? If so, then we can predict that for every data line, the anchor will be replicated one time for each key column.
	YES, the variable NAMES are what is referred to as the key column.



There are three interrelated rules which make a dataset tidy:
```
    Each variable must have its own column.
    Each observation must have its own row.
    Each value must have its own cell.
```

Use JOIN to take two separate tables of data and combine them. Use GATHER to reorganize existing data.


complete() takes a set of columns, and finds all unique combinations. It then ensures the original dataset contains all those values, filling in explicit NAs where necessary.

Thereâ€™s one other important tool that you should know for working with missing values. Sometimes when a data source has primarily been used for data entry, missing values indicate that the previous value should be carried forward:

treatment <- tribble(
  ~ person,           ~ treatment, ~response,
  "Derrick Whitmore", 1,           7,
  NA,                 2,           10,
  NA,                 3,           9,
  "Katherine Burke",  1,           4
)

You can fill in these missing values with fill(). It takes a set of columns where you want missing values to be replaced by the most recent non-missing value (sometimes called last observation carried forward).





Amazon Redshift

Use R Plumber to build API for database access.
https://www.rplumber.io/


Use mutate and row_number to create TA and studnt name substitutions, then  join to combine and anonymize.

Classify the texts I have already using Naive Bayes

Book: Teacher Written Commentary
Lynn Goldstein

https://www.press.umich.edu/6737/teacher_written_commentary_in_second_language_writing_classrooms


  

***

##IN PROGRESS: De-Identify TAs and Students in base_data
For this step I need to mutate "base_data" to create a new file where TA names and student email addresses are replaced with pre-assigned de-identifiers.

I have created a set of uniform anonymous identifiers and put them in a CSV file. From there I can import them to create lookup tables for de-identifying TAs' names and student email addresses. 

This code block reads in the raw dataset containing TA names, student email addresses, and corresponding anonymous IDs, from CSV file then generates TWO data frames.

```{r}
TA_identifiers <- read_csv(file='data/ID_lookup.csv')[1:15,2:1]
#assumes Cols. 1-2 are the TA_IDs and TA names, and are not the full range of rows, so we only want rows 1-15.
class(TA_identifiers) #Should report as "tbl_df"
```

```{r}
Student_identifiers <- read_csv(file='data/ID_lookup.csv')[,5:4]
#assumes Cols. 4-5 are the Student_IDs and Student email addresses.
class(Student_identifiers) #Should report as "tbl_df"
```

**Everything works up to here, but I cannot get next step of lookup process to work. I tried several methods, and none of the elegant ones seem to work. **

I used this Stack Overflow thread as my starting point:

https://stackoverflow.com/questions/35636315/replace-values-in-a-dataframe-based-on-lookup-table

Text copied from the user who posed problem:  

I am having some trouble replacing values in a dataframe. I would like to replace values based on a separate table. Below is an example of what I am trying to do.

I have a table where every row is a customer and every column is an animal they purchased. Lets call this dataframe Original_table.

> Original_table
#       P1     P2     P3
# 1    cat lizard parrot
# 2 lizard parrot    cat
# 3 parrot    cat lizard

I also have a table that I will reference called lookUp.

> lookUp
#      pet   class
# 1    cat  mammal
# 2 lizard reptile
# 3 parrot    bird

What I want to do is create a new table called new with a function replaces all values in table with the class column in lookUp.

What readers recommended:

1. Use JOIN to take two separate tables of data and combine them. Use GATHER to reorganize existing data. 

```
library(dplyr)
library(tidyr)
Original_table %>%
   gather(key = "pet") %>%
   left_join(lookUp, by = "pet") %>%
   spread(key = pet, value = class)
```
Above mechanism failed. **Unsure how to modify and implement. **

None of the other recommended methods seemed to work either. 


I have this UGLY HACK TO GENERATE WHAT I NEED in a single column tibble. 
1. Create one column of TAs, then using **lapply,** loop over column and match values to the look up table. 
2. Store in list, then convert list to tibble.
3. Unfinished step is to merge back in; that command is missing.

```{r}
deidentified_data2<-base_data$ta #Extract just TA name from base_data

deidentified_data2 <- lapply(base_data$ta, function(x) TA_identifiers$TA_ID[match(x, TA_identifiers$ta)]) #Loop over column and replace.

deidentified_data3 <- tibble(line = seq_along(data), text = deidentified_data2) #tibble-izer; converts the list into a tibble.
```



  
  
```{text}

aes <- 
  readtext(file = "plain_texts/*.txt", # read each file .txt
           docvarsfrom = "filenames", # get attributes from filename
           docvarnames = c("language", "country", "year", "title", "type", "genre", "imdb_id")) # add the column names we want for each attribute

glimpse(aes) # preview structure of the object
```


```{r}
library(tidytext)
library(readtext)
```



```{r}
text_files_list <- readtext(file = "plain_texts/*.txt")
```






word
jane austen = 1. Basic Criteria
author = code.subject
proportion

```{r}
library(scales)

# expect a warning about rows with missing values being removed
ggplot(frequency, aes(x = proportion, y = `Jane Austen`, color = abs(`Jane Austen` - proportion))) +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
  facet_wrap(~author, ncol = 2) +
  theme(legend.position="none") +
  labs(y = "Jane Austen", x = NULL)
```


***

##Summarize Word Frequencies in FULL Set of Comments
Take all comments from base_data, unnest tokens, filter stopwords, count and sort remaining terms, then writing back to a new datatable called 'sortedwords.comments.all'.  

Calculate TOTAL words and store. Then calculate fractional frequencies for each word and store using mutate.


##Summarize Word Frequencies in Subsets of Comments Based on 'Subject'
Take subsets of comments from base_data_tokenized using code.subject as separating characteristic.
```{r}
#Pull SUBSETs of tokenized data based on code.subject: 
base_data_tokenized.basic <- subset(base_data_tokenized, code.subject == "1. Basic Criteria", select = (1:28))
base_data_tokenized.writing <- subset(base_data_tokenized, code.subject == "2. Writing Quality", select = (1:28))
base_data_tokenized.technical <- subset(base_data_tokenized, code.subject == "3. Technical and Scientific", select = (1:28))
base_data_tokenized.logic <- subset(base_data_tokenized, code.subject == "4. Logic and Thinking", select = (1:28))
base_data_tokenized.praise <- subset(base_data_tokenized, code.subject == "5. Praise or Concern", select = (1:28))
base_data_tokenized.misconduct <- subset(base_data_tokenized, code.subject == "6. Misconduct", select = (1:28))
base_data_tokenized.narrative <- subset(base_data_tokenized, code.subject == "12. Narrative Comments", select = (1:28))
base_data_tokenized.sciname <- subset(base_data_tokenized, code.subject == "Scientific Name", select = (1:28))
```


For each subset, filter stopwords, count words and total, then calculate frequencies.

```{r}
sortedwords.basic <- base_data_tokenized.basic %>%
  filter(!word %in% stop_words$word,
         !word %in% str_remove_all(stop_words$word, "'"),
         str_detect(word, "[a-z]")) %>%
  count(word, word, sort = TRUE) %>%
  ungroup()

#Calculate the number of words in total in the subset
total_words.basic <- sortedwords.basic %>% 
  summarize(total = sum(n))

#Mutate table to calculate then append fractional word frequency in the text.
sortedwords.basic.fraction <- sortedwords.basic %>% as_tibble() %>% mutate(
  basic.fraction = (n*100)/total_words.basic$total
)
```

```{r}
sortedwords.writing <- base_data_tokenized.writing %>%
  filter(!word %in% stop_words$word,
         !word %in% str_remove_all(stop_words$word, "'"),
         str_detect(word, "[a-z]")) %>%
  count(word, word, sort = TRUE) %>%
  ungroup()

#Calculate the number of words in total in the subset
total_words.writing <- sortedwords.writing %>% 
  summarize(total = sum(n))

#Mutate table to calculate then append fractional word frequency in the text.
sortedwords.writing.fraction <- sortedwords.writing %>% as_tibble() %>% mutate(
  writing.fraction = (n*100)/total_words.writing$total
)
```




 %>% 
  select(-n) %>% 
  spread(code.subject, proportion) %>% 
  gather(code.subject, proportion, `2. Writing Quality`:`3. Technical and Scientific`:`4. Logic and Thinking`)

```



  group_by(code.subject) %>%
  mutate(proportion = n / sum(n)) %>% 
  select(-n) %>% 
  spread(comment.subject, proportion) %>% 
  gather(comment.subject, proportion, `2. Writing Quality`:`3. Technical and Scientific`:`4. Logic and Thinking`)

In Jane Austen example, term is in "word", and "author" lists Names.

MY data have "word"" in Column 28 of table, and "comment.subject" items in Column 22.

Can pull straight using FILTER in dplyer.

https://dplyr.tidyverse.org/reference/filter.html

```{r}
base_data_tokenized.counts <-- base_data_tokenized %>% 
  count(comment.subject, word, sort=TRUE) 
#FAILS
```







*** 

*** 


##ALL Comments
```{r}
base_data_tokenized <- base_data %>% 
  unnest_tokens(word,ta.comment)

base_data_tokenized2 <-base_data_tokenized %>% 
  anti_join(stop_words) %>% 
  count(word, sort=TRUE)

#Calculate the total number of words in total in the subset
total_words.all <- base_data_tokenized2 %>% 
  summarize(total = sum(n))

#Mutate table. Calculate fractional word frequency in the text, and replace raw count.
base_data_tokenized2 <- base_data_tokenized2 %>% as_tibble() %>% mutate(
  n = (n*100)/total_words.all$total
  )

#Plot words in declining frequency
base_data_tokenized2 %>%
  dplyr::filter(n > 0.5) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab("All Words in All Subjects") +
  ylab("% of all words") +
  coord_flip()
ggsave("Frequency_ALL.png")
```

###Basic Criteria
```{r}
base_data_tokenized2.basic <- subset(base_data_tokenized, code.subject == "1. Basic Criteria", select = (1:28)) %>% 
  anti_join(stop_words) %>% 
  count(word, sort=TRUE)

#Calculate the total number of words in total in the subset
total_words.basic <- base_data_tokenized2.basic %>% 
  summarize(total = sum(n))

#Mutate table. Calculate fractional word frequency in the text, and replace raw count.
base_data_tokenized2.basic <- base_data_tokenized2.basic %>% as_tibble() %>% mutate(
  n = (n*100)/total_words.basic$total
  )

#Plot words in declining frequency
base_data_tokenized2.basic %>%
  dplyr::filter(n > 0.5) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab("Basic Criteria") +
  ylab("% of all words") +
  coord_flip()
ggsave("Frequency_Basic.png")

```

###Writing Quality
```{r}
base_data_tokenized2.writing <- subset(base_data_tokenized, code.subject == "2. Writing Quality", select = (1:28)) %>% 
  anti_join(stop_words) %>% 
  count(word, sort=TRUE)

#Calculate the total number of words in total in the subset
total_words.writing <- base_data_tokenized2.writing %>% 
  summarize(total = sum(n))

#Mutate table. Calculate fractional word frequency in the text, and replace raw count.
base_data_tokenized2.writing <- base_data_tokenized2.writing %>% as_tibble() %>% mutate(
  n = (n*100)/total_words.writing$total
  )

#Plot words in declining frequency
base_data_tokenized2.writing %>%
  dplyr::filter(n > 0.5) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab("Writing") +
  ylab("% of all words") +
  coord_flip()
ggsave("Frequency_Writing.png")

```

###Technical
```{r}
base_data_tokenized2.technical <- subset(base_data_tokenized, code.subject == "3. Technical and Scientific", select = (1:28)) %>% 
  anti_join(stop_words) %>% 
  count(word, sort=TRUE)

#Calculate the total number of words in total in the subset
total_words.technical <- base_data_tokenized2.technical %>% 
  summarize(total = sum(n))

#Mutate table. Calculate fractional word frequency in the text, and replace raw count.
base_data_tokenized2.technical <- base_data_tokenized2.technical %>% as_tibble() %>% mutate(
  n = (n*100)/total_words.technical$total
  )

#Plot words in declining frequency
base_data_tokenized2.basic %>%
  dplyr::filter(n > 0.5) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab("Technical") +
  ylab("% of all words") +
  coord_flip()
ggsave("Frequency_Technical.png")

```

###Logic and Thinking
```{r}
base_data_tokenized2.logic <- subset(base_data_tokenized, code.subject == "4. Logic and Thinking", select = (1:28)) %>% 
  anti_join(stop_words) %>% 
  count(word, sort=TRUE)

#Calculate the total number of words in total in the subset
total_words.logic <- base_data_tokenized2.logic %>% 
  summarize(total = sum(n))

#Mutate table. Calculate fractional word frequency in the text, and replace raw count.
base_data_tokenized2.logic <- base_data_tokenized2.logic %>% as_tibble() %>% mutate(
  n = (n*100)/total_words.logic$total
  )

#Plot words in declining frequency
base_data_tokenized2.logic %>%
  dplyr::filter(n > 0.5) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab("Logic") +
  ylab("% of all words") +
  coord_flip()
ggsave("Frequency_Logic.png")

```

###Praise or Concern
```{r}
base_data_tokenized2.praise <- subset(base_data_tokenized, code.subject == "5. Praise or Concern", select = (1:28)) %>% 
  anti_join(stop_words) %>% 
  count(word, sort=TRUE)

#Calculate the total number of words in total in the subset
total_words.praise <- base_data_tokenized2.praise %>% 
  summarize(total = sum(n))

#Mutate table. Calculate fractional word frequency in the text, and replace raw count.
base_data_tokenized2.praise <- base_data_tokenized2.praise %>% as_tibble() %>% mutate(
  n = (n*100)/total_words.praise$total
  )

#Plot words in declining frequency
base_data_tokenized2.praise %>%
  dplyr::filter(n > 0.5) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab("Praise") +
  ylab("% of all words") +
  coord_flip()
ggsave("Frequency_Praise.png")

```

###Misconduct
```{r}
base_data_tokenized2.misconduct <- subset(base_data_tokenized, code.subject == "6. Misconduct", select = (1:28)) %>% 
  anti_join(stop_words) %>% 
  count(word, sort=TRUE)

#Calculate the total number of words in total in the subset
total_words.misconduct <- base_data_tokenized2.misconduct %>% 
  summarize(total = sum(n))

#Mutate table. Calculate fractional word frequency in the text, and replace raw count.
base_data_tokenized2.misconduct <- base_data_tokenized2.misconduct %>% as_tibble() %>% mutate(
  n = (n*100)/total_words.misconduct$total
  )

#Plot words in declining frequency
base_data_tokenized2.misconduct %>%
  dplyr::filter(n > 0.1) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab("Misconduct") +
  ylab("% of all words") +
  coord_flip()
ggsave("Frequency_Misconduct.png")

```

###Narrative
```{r}
base_data_tokenized2.narrative <- subset(base_data_tokenized, code.subject == "12. Narrative Comments", select = (1:28)) %>% 
  anti_join(stop_words) %>% 
  count(word, sort=TRUE)

#Calculate the total number of words in total in the subset
total_words.narrative <- base_data_tokenized2.narrative %>% 
  summarize(total = sum(n))

#Mutate table. Calculate fractional word frequency in the text, and replace raw count.
base_data_tokenized2.narrative <- base_data_tokenized2.narrative %>% as_tibble() %>% mutate(
  n = (n*100)/total_words.narrative$total
  )

#Plot words in declining frequency
base_data_tokenized2.narrative %>%
  dplyr::filter(n > 0.5) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab("Narrative") +
  ylab("% of all words") +
  coord_flip()
ggsave("Frequency_Narrative.png")

```

###Scientific Names
```{r}
base_data_tokenized2.sciname <- subset(base_data_tokenized, code.subject == "Scientific Name", select = (1:28)) %>% 
  anti_join(stop_words) %>% 
  count(word, sort=TRUE)

#Calculate the total number of words in total in the subset
total_words.sciname <- base_data_tokenized2.sciname %>% 
  summarize(total = sum(n))

#Mutate table. Calculate fractional word frequency in the text, and replace raw count.
base_data_tokenized2.sciname <- base_data_tokenized2.sciname %>% as_tibble() %>% mutate(
  n = (n*100)/total_words.sciname$total
  )

#Plot words in declining frequency
base_data_tokenized2.sciname %>%
  dplyr::filter(n > 0.5) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab("SciNames") +
  ylab("% of all words") +
  coord_flip()
ggsave("Frequency_SciName.png")

```




***
***

##Summarize Word Frequencies in FULL Set of Comments
Take all comments from base_data, unnest tokens, filter stopwords, count and sort remaining terms, then writing back to a new datatable called 'sortedwords.comments.all'.  

Calculate TOTAL words and store. Then calculate fractional frequencies for each word and store using mutate.

```{r}
library(dplyr)
library(tidytext)

sortedwords.comments.all <- base_data_tokenized %>%
  filter(!word %in% stop_words$word,
         !word %in% str_remove_all(stop_words$word, "'"),
         str_detect(word, "[a-z]")) %>%
  count(word, word, sort = TRUE) %>%
  ungroup()

#Calculate the number of words in total in the subset
total_words.all <- sortedwords.comments.all %>% 
  summarize(total = sum(n))

#Mutate table to calculate then append fractional word frequency in the text.
sortedwords.comments.all.fraction <- sortedwords.comments.all %>% as_tibble() %>% mutate(
  all.fraction = (n*100)/total_words.all$total
  )
```


##Summarize Word Frequencies in Subsets of Comments Based on 'Subject'
Take subsets of comments from base_data_tokenized using code.subject as separating characteristic.
```{r}
#Pull SUBSETs of tokenized data based on code.subject: 
base_data_tokenized.basic <- subset(base_data_tokenized, code.subject == "1. Basic Criteria", select = (1:28))
base_data_tokenized.writing <- subset(base_data_tokenized, code.subject == "2. Writing Quality", select = (1:28))
base_data_tokenized.technical <- subset(base_data_tokenized, code.subject == "3. Technical and Scientific", select = (1:28))
base_data_tokenized.logic <- subset(base_data_tokenized, code.subject == "4. Logic and Thinking", select = (1:28))
base_data_tokenized.praise <- subset(base_data_tokenized, code.subject == "5. Praise or Concern", select = (1:28))
base_data_tokenized.misconduct <- subset(base_data_tokenized, code.subject == "6. Misconduct", select = (1:28))
base_data_tokenized.narrative <- subset(base_data_tokenized, code.subject == "12. Narrative Comments", select = (1:28))
base_data_tokenized.sciname <- subset(base_data_tokenized, code.subject == "Scientific Name", select = (1:28))
```


For each subset, filter stopwords, count words and total, then calculate frequencies.

```{r}
sortedwords.basic <- base_data_tokenized.basic %>%
  filter(!word %in% stop_words$word,
         !word %in% str_remove_all(stop_words$word, "'"),
         str_detect(word, "[a-z]")) %>%
  count(word, word, sort = TRUE) %>%
  ungroup()

#Calculate the number of words in total in the subset
total_words.basic <- sortedwords.basic %>% 
  summarize(total = sum(n))

#Mutate table to calculate then append fractional word frequency in the text.
sortedwords.basic.fraction <- sortedwords.basic %>% as_tibble() %>% mutate(
  basic.fraction = (n*100)/total_words.basic$total
)
```

```{r}
sortedwords.writing <- base_data_tokenized.writing %>%
  filter(!word %in% stop_words$word,
         !word %in% str_remove_all(stop_words$word, "'"),
         str_detect(word, "[a-z]")) %>%
  count(word, word, sort = TRUE) %>%
  ungroup()

#Calculate the number of words in total in the subset
total_words.writing <- sortedwords.writing %>% 
  summarize(total = sum(n))

#Mutate table to calculate then append fractional word frequency in the text.
sortedwords.writing.fraction <- sortedwords.writing %>% as_tibble() %>% mutate(
  writing.fraction = (n*100)/total_words.writing$total
)
```

```{r}
sortedwords.technical <- base_data_tokenized.technical %>%
  filter(!word %in% stop_words$word,
         !word %in% str_remove_all(stop_words$word, "'"),
         str_detect(word, "[a-z]")) %>%
  count(word, word, sort = TRUE) %>%
  ungroup()

#Calculate the number of words in total in the subset
total_words.technical <- sortedwords.technical %>% 
  summarize(total = sum(n))

#Mutate table to calculate then append fractional word frequency in the text.
sortedwords.technical.fraction <- sortedwords.technical %>% as_tibble() %>% mutate(
  technical.fraction = (n*100)/total_words.technical$total
)
```

```{r}
sortedwords.logic <- base_data_tokenized.logic %>%
  filter(!word %in% stop_words$word,
         !word %in% str_remove_all(stop_words$word, "'"),
         str_detect(word, "[a-z]")) %>%
  count(word, word, sort = TRUE) %>%
  ungroup()

#Calculate the number of words in total in the subset
total_words.logic <- sortedwords.logic %>% 
  summarize(total = sum(n))

#Mutate table to calculate then append fractional word frequency in the text.
sortedwords.logic.fraction <- sortedwords.logic %>% as_tibble() %>% mutate(
  logic.fraction = (n*100)/total_words.logic$total
)
```

Join the individual data tables for each subset in a larger file using column named 'word'. Columns get confusing names so they are renamed.

**WARNING**: Words that are MISSING from **any subset** cause the full set to be pruned. I am losing the unique words this way. **Need to figure out how to join without pruning.**


```{r}
total <- merge(sortedwords.comments.all.fraction,sortedwords.writing.fraction,by="word") 
total2 <- merge(total,sortedwords.technical.fraction,by="word")
total3 <- merge(total2,sortedwords.logic.fraction,by="word")
#rename lines that were possibly confusing
names(total3)[2] <- "all.count"
names(total3)[4] <- "writing.count"
names(total3)[6] <- "technical.count"
names(total3)[8] <- "logic.count"

#Mutate table to calculate then append fractional word frequency in the text.
total4 <- total3 %>% as_tibble() %>% mutate(
  writing.wt = (writing.count *100)/all.count
)

total5 <- total4 %>% as_tibble() %>% mutate(
  technical.wt = (technical.count *100)/all.count
)

total6 <- total5 %>% as_tibble() %>% mutate(
  logic.wt = (logic.count *100)/all.count
)

```


To Do Next:

Now I need to cross-graph the weights for each word relative to others. 



***
***


###Alternative Options
For other types of analyses I might want to extract text using other methods shown below:

**Option 1.**  Create a single-column data frame of just the comments. Then check that the data was written to the file as characters not factors.
```{r}
base_data_comments <- as.data.frame(base_data[,22], drop=FALSE, stringsAsFactors=FALSE) #assumes Col. 22 is the ta.commment column
class(base_data_comments[23,1]) #Should report as "character"
```

**Option 2.** Create a NEW data frame for analysis. Contains **ta.comment** (Col. 22), **code.subject** (Col. 23) and **code.structure** (Col. 24). Again, check that written in data are characters, not factors. 
```{r}
base_data_3columns <- as.data.frame(base_data[,c(22:24)], drop=FALSE, stringsAsFactors=FALSE)
(base_data_3columns[33:35,1:3]) 
#First column should be row numbers, 2nd to 4th columns of class "chr"
```

**Option 3.** Import comments as subset blocks
This is an ugly set of steps to create dataframes for analysis. The output subsets of data from the full base_data table can be analyzed as separate groups. Only benefit I see is they are COMPLETELY INDEPENDENT of one another.
```{r}
#The columns to be included in each subset table
colnames.base_data <-  c("unique.record","ta.comment","code.subject","code.structure")
# These first lines create a subset table for ALL lines in the table.
base_data_3columns.all <- subset(base_data, select = colnames.base_data)
base_data_comments.all <- as.data.frame(base_data_3columns.all[,2], drop=FALSE, stringsAsFactors=FALSE) #assumes Col. 2 is the ta.commment column

# The first line of each pair from here down creates subset tables based on what is in the "code.subject" data column. The second line of each pair extracts just the TA comments so they can be tokenized.
base_data_3columns.basic <- subset(base_data, code.subject == "1. Basic Criteria", select = colnames.base_data)
base_data_comments.basic <- as.data.frame(base_data_3columns.basic[,2], drop=FALSE, stringsAsFactors=FALSE) #assumes Col. 2 is the ta.commment column

base_data_3columns.writing <- subset(base_data, code.subject == "2. Writing Quality", select = colnames.base_data)
base_data_comments.writing <- as.data.frame(base_data_3columns.writing[,2], drop=FALSE, stringsAsFactors=FALSE) #assumes Col. 2 is the ta.commment column

base_data_3columns.technical <- subset(base_data, code.subject == "3. Technical and Scientific", select = colnames.base_data)
base_data_comments.technical <- as.data.frame(base_data_3columns.technical[,2], drop=FALSE, stringsAsFactors=FALSE) #assumes Col. 2 is the ta.commment column

base_data_3columns.logic <- subset(base_data, code.subject == "4. Logic and Thinking", select = colnames.base_data)
base_data_comments.logic <- as.data.frame(base_data_3columns.logic[,2], drop=FALSE, stringsAsFactors=FALSE) #assumes Col. 2 is the ta.commment column

base_data_3columns.praise <- subset(base_data, code.subject == "5. Praise or Concern", select = colnames.base_data)
base_data_comments.praise <- as.data.frame(base_data_3columns.praise[,2], drop=FALSE, stringsAsFactors=FALSE) #assumes Col. 2 is the ta.commment column

base_data_3columns.misconduct <- subset(base_data, code.subject == "6. Misconduct", select = colnames.base_data)
base_data_comments.misconduct <- as.data.frame(base_data_3columns.misconduct[,2], drop=FALSE, stringsAsFactors=FALSE) #assumes Col. 2 is the ta.commment column

base_data_3columns.narrative <- subset(base_data, code.subject == "12. Narrative Comments", select = colnames.base_data)
base_data_comments.narrative <- as.data.frame(base_data_3columns.narrative[,2], drop=FALSE, stringsAsFactors=FALSE) #assumes Col. 2 is the ta.commment column

base_data_3columns.sciname <- subset(base_data, code.subject == "Scientific Name", select =  colnames.base_data)
base_data_comments.sciname <- as.data.frame(base_data_3columns.sciname[,2], drop=FALSE, stringsAsFactors=FALSE) #assumes Col. 2 is the ta.commment column
```

This second series of commands creates tokenized words lists for all comments together, and token subsets for each comments subset.
```{r}
base_data_comments.all.tokens <- base_data_comments %>% 
  unnest_tokens(word,ta.comment)
base_data_comments.basic.tokens <- base_data_comments.basic %>% 
  unnest_tokens(word,ta.comment)
base_data_comments.writing.tokens <- base_data_comments.writing %>% 
  unnest_tokens(word,ta.comment)
base_data_comments.technical.tokens <- base_data_comments.technical %>% 
  unnest_tokens(word,ta.comment)
base_data_comments.logic.tokens <- base_data_comments.logic %>% 
  unnest_tokens(word,ta.comment)
base_data_comments.praise.tokens <- base_data_comments.praise %>% 
  unnest_tokens(word,ta.comment)
base_data_comments.misconduct.tokens <- base_data_comments.misconduct %>% 
  unnest_tokens(word,ta.comment)
base_data_comments.narrative.tokens <- base_data_comments.narrative %>% 
  unnest_tokens(word,ta.comment)
base_data_comments.sciname.tokens <- base_data_comments.sciname %>% 
  unnest_tokens(word,ta.comment)

#Display the last set of tokens to show process finished.
base_data_comments.sciname.tokens
```


***

# Tokenizing Text of Comments
Create a single-column data frame of just the comments for ALL lines. Then check that the data was written to the file as characters not factors.



####If you must convert data to a Tibble:
To make tokenizing work you must convert the desired set of extracted data into a tibble. You HAVE to use a tibble to tokenize text lines. Trying to do it straight from a values table will throw an error.

```
base_data_comments_50_tibble <- tibble(line = seq_along(data), text = base_data_comments_50(c[,2])) #tibble-izer

#base_data_comments_50_tibble %>% 
#  unnest_tokens(word,ta.comment)
```
unique.record, code.subject, code.structure
base_data


***
***

##FWIW
This command uses a tibble with 3 data columns, and successfully unnests the tokens. It is messy because the other columns are duplicated, BUT at least it is possible to wrangle a multi-column tibble for tokenization.
```{r}
Abase_data_3columns.sciname.tokens <- base_data_3columns.sciname %>% 
  unnest_tokens(word,ta.comment)
```


