---
title: "Round 2 for Data Intake Method"
author: "Dan Johnson"
date: "11/24/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



##Preparing and Pre-Checking the Initial Dataset
Call in tidyverse library, then read in the raw dataset from .CSV file to a data frame with name "base_data".

```{r Libraries}
library(tidyverse)
library(tidytext)
library(dplyr)
library(tidyr)
library(ggplot2)

base_data <- read_csv(file='data/coded_full_comments_dataset_Spring18.csv')
head(base_data)
```

Already HAVE text correctly organized. Just need to filter and pull rows I want.


```{r}
# This pulls the table rows I want to compare.
frequency <- filter(base_data, code.subject=="1. Basic Criteria" |code.subject=="2. Writing Quality"|code.subject=="3. Technical and Scientific"|code.subject=="4. Logic and Thinking")

# This step tokenizes phrases, removes stop words
base_data_tokenized <- frequency %>% 
  unnest_tokens(word,ta.comment) %>% 
  anti_join(stop_words)
```

```{r}
#Groups by code.subject
base_data_tokenized_sorted <- base_data_tokenized



```



#sort by mpg (ascending) and cyl (descending)
newdata <- mtcars[order(mpg, -cyl),]



  group_by(comment.subject) %>%
  mutate(proportion = n / sum(n)) %>% 
  select(-n) %>% 
  spread(comment.subject, proportion) %>% 
  gather(comment.subject, proportion, `2. Writing Quality`:`3. Technical and Scientific`:`4. Logic and Thinking`)

In Jane Austen example, term is in "word", and "author" lists Names.

MY data have "word"" in Column 28 of table, and "comment.subject" items in Column 22.

Can pull straight using FILTER in dplyer.

https://dplyr.tidyverse.org/reference/filter.html

```{r}
base_data_tokenized.counts <-- base_data_tokenized %>% 
  count(comment.subject, word, sort=TRUE) 
#FAILS
```





```{r}
sortedwords.comments.all <- base_data_tokenized %>%
  filter(!word %in% stop_words$word,
         !word %in% str_remove_all(stop_words$word, "'"),
         str_detect(word, "[a-z]")) %>%
  count(word, word, sort = TRUE) %>%
  ungroup()


#Calculate the number of words in total in the subset
total_words.all <- sortedwords.comments.all %>% 
  summarize(total = sum(n))


#Mutate table to calculate then append fractional word frequency in the text.
sortedwords.comments.all.fraction <- sortedwords.comments.all %>% as_tibble() %>% mutate(
  all.fraction = (n*100)/total_words.all$total
  )
#Works
```




sort(unique(base_data_tokenized$code.subject))


##Summarize Word Frequencies in FULL Set of Comments
Take all comments from base_data, unnest tokens, filter stopwords, count and sort remaining terms, then writing back to a new datatable called 'sortedwords.comments.all'.  

Calculate TOTAL words and store. Then calculate fractional frequencies for each word and store using mutate.


##Summarize Word Frequencies in Subsets of Comments Based on 'Subject'
Take subsets of comments from base_data_tokenized using code.subject as separating characteristic.
```{r}
#Pull SUBSETs of tokenized data based on code.subject: 
base_data_tokenized.basic <- subset(base_data_tokenized, code.subject == "1. Basic Criteria", select = (1:28))
base_data_tokenized.writing <- subset(base_data_tokenized, code.subject == "2. Writing Quality", select = (1:28))
base_data_tokenized.technical <- subset(base_data_tokenized, code.subject == "3. Technical and Scientific", select = (1:28))
base_data_tokenized.logic <- subset(base_data_tokenized, code.subject == "4. Logic and Thinking", select = (1:28))
base_data_tokenized.praise <- subset(base_data_tokenized, code.subject == "5. Praise or Concern", select = (1:28))
base_data_tokenized.misconduct <- subset(base_data_tokenized, code.subject == "6. Misconduct", select = (1:28))
base_data_tokenized.narrative <- subset(base_data_tokenized, code.subject == "12. Narrative Comments", select = (1:28))
base_data_tokenized.sciname <- subset(base_data_tokenized, code.subject == "Scientific Name", select = (1:28))
```


For each subset, filter stopwords, count words and total, then calculate frequencies.

```{r}
sortedwords.basic <- base_data_tokenized.basic %>%
  filter(!word %in% stop_words$word,
         !word %in% str_remove_all(stop_words$word, "'"),
         str_detect(word, "[a-z]")) %>%
  count(word, word, sort = TRUE) %>%
  ungroup()

#Calculate the number of words in total in the subset
total_words.basic <- sortedwords.basic %>% 
  summarize(total = sum(n))

#Mutate table to calculate then append fractional word frequency in the text.
sortedwords.basic.fraction <- sortedwords.basic %>% as_tibble() %>% mutate(
  basic.fraction = (n*100)/total_words.basic$total
)
```

```{r}
sortedwords.writing <- base_data_tokenized.writing %>%
  filter(!word %in% stop_words$word,
         !word %in% str_remove_all(stop_words$word, "'"),
         str_detect(word, "[a-z]")) %>%
  count(word, word, sort = TRUE) %>%
  ungroup()

#Calculate the number of words in total in the subset
total_words.writing <- sortedwords.writing %>% 
  summarize(total = sum(n))

#Mutate table to calculate then append fractional word frequency in the text.
sortedwords.writing.fraction <- sortedwords.writing %>% as_tibble() %>% mutate(
  writing.fraction = (n*100)/total_words.writing$total
)
```







%>%
  group_by(comment.subject) %>%
  mutate(proportion = n / sum(n)) %>% 
  select(-n) %>% 
  spread(comment.subject, proportion) %>% 
  gather(comment.subject, proportion, `2. Writing Quality`:`3. Technical and Scientific`:`4. Logic and Thinking`)
```

tidy_books %>%
  count(word, sort = TRUE) 

library(tidyr)

frequency <- bind_rows(mutate(tidy_bronte, author = "Brontë Sisters"),
                       mutate(tidy_hgwells, author = "H.G. Wells"), 
                       mutate(tidy_books, author = "Jane Austen")) %>% 
  mutate(word = str_extract(word, "[a-z']+")) %>%
  count(author, word) %>%
  group_by(author) %>%
  mutate(proportion = n / sum(n)) %>% 
  select(-n) %>% 
  spread(author, proportion) %>% 
  gather(author, proportion, `Brontë Sisters`:`H.G. Wells`)

```

```{r}
library(scales)

# expect a warning about rows with missing values being removed
ggplot(frequency, aes(x = proportion, y = `Jane Austen`, color = abs(`Jane Austen` - proportion))) +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
  facet_wrap(~author, ncol = 2) +
  theme(legend.position="none") +
  labs(y = "Jane Austen", x = NULL)
```


```{r}
base_data_tokenized %>% 
  mutate(word = str_extract(word, "[a-z']+")) %>%
  count(code.subject, word) %>%
  group_by(code.subject) %>% 
  mutate(proportion = n/sum(n)) %>% 
  select(-n) %>% spread(code.subject, proportion) %>% 
  gather(code.subject, proportion,`2. Writing Quality`:`3. Technical and Scientific`:`4. Logic and Thinking`)

base_data_tokenized
```


```{r}
library(scales)

ggplot(frequency, aes(x = proportion, y = n, color = abs(`Jane Austen` - proportion))) +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
  facet_wrap(~author, ncol = 2) +
  theme(legend.position="none") +
  labs(y = "Jane Austen", x = NULL)
```

 
 
  
    count(word, sort=TRUE)

#Calculate the total number of words in total in the subset
total_words.all <- base_data_tokenized2 %>% 
  summarize(total = sum(n))

#Mutate table. Calculate fractional word frequency in the text, and replace raw count.
base_data_tokenized2 <- base_data_tokenized2 %>% as_tibble() %>% mutate(
  n = (n*100)/total_words.all$total
  )

#Plot words in declining frequency
base_data_tokenized2 %>%
  dplyr::filter(n > 0.5) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab("All Words in All Subjects") +
  ylab("% of all words") +
  coord_flip()
ggsave("Frequency_ALL.png")
```



```{r}
library(dplyr)
library(tidytext)

sortedwords.comments.all <- base_data_tokenized %>%
  filter(!word %in% stop_words$word,
         !word %in% str_remove_all(stop_words$word, "'"),
         str_detect(word, "[a-z]")) %>%
  count(word, word, sort = TRUE) %>%
  ungroup()

#Calculate the number of words in total in the subset
total_words.all <- sortedwords.comments.all %>% 
  summarize(total = sum(n))

#Mutate table to calculate then append fractional word frequency in the text.
sortedwords.comments.all.fraction <- sortedwords.comments.all %>% as_tibble() %>% mutate(
  all.fraction = (n*100)/total_words.all$total
  )
```





*** 

*** 


##ALL Comments
```{r}
base_data_tokenized <- base_data %>% 
  unnest_tokens(word,ta.comment)

base_data_tokenized2 <-base_data_tokenized %>% 
  anti_join(stop_words) %>% 
  count(word, sort=TRUE)

#Calculate the total number of words in total in the subset
total_words.all <- base_data_tokenized2 %>% 
  summarize(total = sum(n))

#Mutate table. Calculate fractional word frequency in the text, and replace raw count.
base_data_tokenized2 <- base_data_tokenized2 %>% as_tibble() %>% mutate(
  n = (n*100)/total_words.all$total
  )

#Plot words in declining frequency
base_data_tokenized2 %>%
  dplyr::filter(n > 0.5) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab("All Words in All Subjects") +
  ylab("% of all words") +
  coord_flip()
ggsave("Frequency_ALL.png")
```

###Basic Criteria
```{r}
base_data_tokenized2.basic <- subset(base_data_tokenized, code.subject == "1. Basic Criteria", select = (1:28)) %>% 
  anti_join(stop_words) %>% 
  count(word, sort=TRUE)

#Calculate the total number of words in total in the subset
total_words.basic <- base_data_tokenized2.basic %>% 
  summarize(total = sum(n))

#Mutate table. Calculate fractional word frequency in the text, and replace raw count.
base_data_tokenized2.basic <- base_data_tokenized2.basic %>% as_tibble() %>% mutate(
  n = (n*100)/total_words.basic$total
  )

#Plot words in declining frequency
base_data_tokenized2.basic %>%
  dplyr::filter(n > 0.5) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab("Basic Criteria") +
  ylab("% of all words") +
  coord_flip()
ggsave("Frequency_Basic.png")

```

###Writing Quality
```{r}
base_data_tokenized2.writing <- subset(base_data_tokenized, code.subject == "2. Writing Quality", select = (1:28)) %>% 
  anti_join(stop_words) %>% 
  count(word, sort=TRUE)

#Calculate the total number of words in total in the subset
total_words.writing <- base_data_tokenized2.writing %>% 
  summarize(total = sum(n))

#Mutate table. Calculate fractional word frequency in the text, and replace raw count.
base_data_tokenized2.writing <- base_data_tokenized2.writing %>% as_tibble() %>% mutate(
  n = (n*100)/total_words.writing$total
  )

#Plot words in declining frequency
base_data_tokenized2.writing %>%
  dplyr::filter(n > 0.5) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab("Writing") +
  ylab("% of all words") +
  coord_flip()
ggsave("Frequency_Writing.png")

```

###Technical
```{r}
base_data_tokenized2.technical <- subset(base_data_tokenized, code.subject == "3. Technical and Scientific", select = (1:28)) %>% 
  anti_join(stop_words) %>% 
  count(word, sort=TRUE)

#Calculate the total number of words in total in the subset
total_words.technical <- base_data_tokenized2.technical %>% 
  summarize(total = sum(n))

#Mutate table. Calculate fractional word frequency in the text, and replace raw count.
base_data_tokenized2.technical <- base_data_tokenized2.technical %>% as_tibble() %>% mutate(
  n = (n*100)/total_words.technical$total
  )

#Plot words in declining frequency
base_data_tokenized2.basic %>%
  dplyr::filter(n > 0.5) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab("Technical") +
  ylab("% of all words") +
  coord_flip()
ggsave("Frequency_Technical.png")

```

###Logic and Thinking
```{r}
base_data_tokenized2.logic <- subset(base_data_tokenized, code.subject == "4. Logic and Thinking", select = (1:28)) %>% 
  anti_join(stop_words) %>% 
  count(word, sort=TRUE)

#Calculate the total number of words in total in the subset
total_words.logic <- base_data_tokenized2.logic %>% 
  summarize(total = sum(n))

#Mutate table. Calculate fractional word frequency in the text, and replace raw count.
base_data_tokenized2.logic <- base_data_tokenized2.logic %>% as_tibble() %>% mutate(
  n = (n*100)/total_words.logic$total
  )

#Plot words in declining frequency
base_data_tokenized2.logic %>%
  dplyr::filter(n > 0.5) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab("Logic") +
  ylab("% of all words") +
  coord_flip()
ggsave("Frequency_Logic.png")

```

###Praise or Concern
```{r}
base_data_tokenized2.praise <- subset(base_data_tokenized, code.subject == "5. Praise or Concern", select = (1:28)) %>% 
  anti_join(stop_words) %>% 
  count(word, sort=TRUE)

#Calculate the total number of words in total in the subset
total_words.praise <- base_data_tokenized2.praise %>% 
  summarize(total = sum(n))

#Mutate table. Calculate fractional word frequency in the text, and replace raw count.
base_data_tokenized2.praise <- base_data_tokenized2.praise %>% as_tibble() %>% mutate(
  n = (n*100)/total_words.praise$total
  )

#Plot words in declining frequency
base_data_tokenized2.praise %>%
  dplyr::filter(n > 0.5) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab("Praise") +
  ylab("% of all words") +
  coord_flip()
ggsave("Frequency_Praise.png")

```

###Misconduct
```{r}
base_data_tokenized2.misconduct <- subset(base_data_tokenized, code.subject == "6. Misconduct", select = (1:28)) %>% 
  anti_join(stop_words) %>% 
  count(word, sort=TRUE)

#Calculate the total number of words in total in the subset
total_words.misconduct <- base_data_tokenized2.misconduct %>% 
  summarize(total = sum(n))

#Mutate table. Calculate fractional word frequency in the text, and replace raw count.
base_data_tokenized2.misconduct <- base_data_tokenized2.misconduct %>% as_tibble() %>% mutate(
  n = (n*100)/total_words.misconduct$total
  )

#Plot words in declining frequency
base_data_tokenized2.misconduct %>%
  dplyr::filter(n > 0.1) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab("Misconduct") +
  ylab("% of all words") +
  coord_flip()
ggsave("Frequency_Misconduct.png")

```

###Narrative
```{r}
base_data_tokenized2.narrative <- subset(base_data_tokenized, code.subject == "12. Narrative Comments", select = (1:28)) %>% 
  anti_join(stop_words) %>% 
  count(word, sort=TRUE)

#Calculate the total number of words in total in the subset
total_words.narrative <- base_data_tokenized2.narrative %>% 
  summarize(total = sum(n))

#Mutate table. Calculate fractional word frequency in the text, and replace raw count.
base_data_tokenized2.narrative <- base_data_tokenized2.narrative %>% as_tibble() %>% mutate(
  n = (n*100)/total_words.narrative$total
  )

#Plot words in declining frequency
base_data_tokenized2.narrative %>%
  dplyr::filter(n > 0.5) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab("Narrative") +
  ylab("% of all words") +
  coord_flip()
ggsave("Frequency_Narrative.png")

```

###Scientific Names
```{r}
base_data_tokenized2.sciname <- subset(base_data_tokenized, code.subject == "Scientific Name", select = (1:28)) %>% 
  anti_join(stop_words) %>% 
  count(word, sort=TRUE)

#Calculate the total number of words in total in the subset
total_words.sciname <- base_data_tokenized2.sciname %>% 
  summarize(total = sum(n))

#Mutate table. Calculate fractional word frequency in the text, and replace raw count.
base_data_tokenized2.sciname <- base_data_tokenized2.sciname %>% as_tibble() %>% mutate(
  n = (n*100)/total_words.sciname$total
  )

#Plot words in declining frequency
base_data_tokenized2.sciname %>%
  dplyr::filter(n > 0.5) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab("SciNames") +
  ylab("% of all words") +
  coord_flip()
ggsave("Frequency_SciName.png")

```




***
***

##Summarize Word Frequencies in FULL Set of Comments
Take all comments from base_data, unnest tokens, filter stopwords, count and sort remaining terms, then writing back to a new datatable called 'sortedwords.comments.all'.  

Calculate TOTAL words and store. Then calculate fractional frequencies for each word and store using mutate.

```{r}
library(dplyr)
library(tidytext)

sortedwords.comments.all <- base_data_tokenized %>%
  filter(!word %in% stop_words$word,
         !word %in% str_remove_all(stop_words$word, "'"),
         str_detect(word, "[a-z]")) %>%
  count(word, word, sort = TRUE) %>%
  ungroup()

#Calculate the number of words in total in the subset
total_words.all <- sortedwords.comments.all %>% 
  summarize(total = sum(n))

#Mutate table to calculate then append fractional word frequency in the text.
sortedwords.comments.all.fraction <- sortedwords.comments.all %>% as_tibble() %>% mutate(
  all.fraction = (n*100)/total_words.all$total
  )
```


##Summarize Word Frequencies in Subsets of Comments Based on 'Subject'
Take subsets of comments from base_data_tokenized using code.subject as separating characteristic.
```{r}
#Pull SUBSETs of tokenized data based on code.subject: 
base_data_tokenized.basic <- subset(base_data_tokenized, code.subject == "1. Basic Criteria", select = (1:28))
base_data_tokenized.writing <- subset(base_data_tokenized, code.subject == "2. Writing Quality", select = (1:28))
base_data_tokenized.technical <- subset(base_data_tokenized, code.subject == "3. Technical and Scientific", select = (1:28))
base_data_tokenized.logic <- subset(base_data_tokenized, code.subject == "4. Logic and Thinking", select = (1:28))
base_data_tokenized.praise <- subset(base_data_tokenized, code.subject == "5. Praise or Concern", select = (1:28))
base_data_tokenized.misconduct <- subset(base_data_tokenized, code.subject == "6. Misconduct", select = (1:28))
base_data_tokenized.narrative <- subset(base_data_tokenized, code.subject == "12. Narrative Comments", select = (1:28))
base_data_tokenized.sciname <- subset(base_data_tokenized, code.subject == "Scientific Name", select = (1:28))
```


For each subset, filter stopwords, count words and total, then calculate frequencies.

```{r}
sortedwords.basic <- base_data_tokenized.basic %>%
  filter(!word %in% stop_words$word,
         !word %in% str_remove_all(stop_words$word, "'"),
         str_detect(word, "[a-z]")) %>%
  count(word, word, sort = TRUE) %>%
  ungroup()

#Calculate the number of words in total in the subset
total_words.basic <- sortedwords.basic %>% 
  summarize(total = sum(n))

#Mutate table to calculate then append fractional word frequency in the text.
sortedwords.basic.fraction <- sortedwords.basic %>% as_tibble() %>% mutate(
  basic.fraction = (n*100)/total_words.basic$total
)
```

```{r}
sortedwords.writing <- base_data_tokenized.writing %>%
  filter(!word %in% stop_words$word,
         !word %in% str_remove_all(stop_words$word, "'"),
         str_detect(word, "[a-z]")) %>%
  count(word, word, sort = TRUE) %>%
  ungroup()

#Calculate the number of words in total in the subset
total_words.writing <- sortedwords.writing %>% 
  summarize(total = sum(n))

#Mutate table to calculate then append fractional word frequency in the text.
sortedwords.writing.fraction <- sortedwords.writing %>% as_tibble() %>% mutate(
  writing.fraction = (n*100)/total_words.writing$total
)
```

```{r}
sortedwords.technical <- base_data_tokenized.technical %>%
  filter(!word %in% stop_words$word,
         !word %in% str_remove_all(stop_words$word, "'"),
         str_detect(word, "[a-z]")) %>%
  count(word, word, sort = TRUE) %>%
  ungroup()

#Calculate the number of words in total in the subset
total_words.technical <- sortedwords.technical %>% 
  summarize(total = sum(n))

#Mutate table to calculate then append fractional word frequency in the text.
sortedwords.technical.fraction <- sortedwords.technical %>% as_tibble() %>% mutate(
  technical.fraction = (n*100)/total_words.technical$total
)
```

```{r}
sortedwords.logic <- base_data_tokenized.logic %>%
  filter(!word %in% stop_words$word,
         !word %in% str_remove_all(stop_words$word, "'"),
         str_detect(word, "[a-z]")) %>%
  count(word, word, sort = TRUE) %>%
  ungroup()

#Calculate the number of words in total in the subset
total_words.logic <- sortedwords.logic %>% 
  summarize(total = sum(n))

#Mutate table to calculate then append fractional word frequency in the text.
sortedwords.logic.fraction <- sortedwords.logic %>% as_tibble() %>% mutate(
  logic.fraction = (n*100)/total_words.logic$total
)
```

Join the individual data tables for each subset in a larger file using column named 'word'. Columns get confusing names so they are renamed.

**WARNING**: Words that are MISSING from **any subset** cause the full set to be pruned. I am losing the unique words this way. **Need to figure out how to join without pruning.**


```{r}
total <- merge(sortedwords.comments.all.fraction,sortedwords.writing.fraction,by="word") 
total2 <- merge(total,sortedwords.technical.fraction,by="word")
total3 <- merge(total2,sortedwords.logic.fraction,by="word")
#rename lines that were possibly confusing
names(total3)[2] <- "all.count"
names(total3)[4] <- "writing.count"
names(total3)[6] <- "technical.count"
names(total3)[8] <- "logic.count"

#Mutate table to calculate then append fractional word frequency in the text.
total4 <- total3 %>% as_tibble() %>% mutate(
  writing.wt = (writing.count *100)/all.count
)

total5 <- total4 %>% as_tibble() %>% mutate(
  technical.wt = (technical.count *100)/all.count
)

total6 <- total5 %>% as_tibble() %>% mutate(
  logic.wt = (logic.count *100)/all.count
)

```


To Do Next:

Now I need to cross-graph the weights for each word relative to others. 



***
***


###Alternative Options
For other types of analyses I might want to extract text using other methods shown below:

**Option 1.**  Create a single-column data frame of just the comments. Then check that the data was written to the file as characters not factors.
```{r}
base_data_comments <- as.data.frame(base_data[,22], drop=FALSE, stringsAsFactors=FALSE) #assumes Col. 22 is the ta.commment column
class(base_data_comments[23,1]) #Should report as "character"
```

**Option 2.** Create a NEW data frame for analysis. Contains **ta.comment** (Col. 22), **code.subject** (Col. 23) and **code.structure** (Col. 24). Again, check that written in data are characters, not factors. 
```{r}
base_data_3columns <- as.data.frame(base_data[,c(22:24)], drop=FALSE, stringsAsFactors=FALSE)
(base_data_3columns[33:35,1:3]) 
#First column should be row numbers, 2nd to 4th columns of class "chr"
```

**Option 3.** Import comments as subset blocks
This is an ugly set of steps to create dataframes for analysis. The output subsets of data from the full base_data table can be analyzed as separate groups. Only benefit I see is they are COMPLETELY INDEPENDENT of one another.
```{r}
#The columns to be included in each subset table
colnames.base_data <-  c("unique.record","ta.comment","code.subject","code.structure")
# These first lines create a subset table for ALL lines in the table.
base_data_3columns.all <- subset(base_data, select = colnames.base_data)
base_data_comments.all <- as.data.frame(base_data_3columns.all[,2], drop=FALSE, stringsAsFactors=FALSE) #assumes Col. 2 is the ta.commment column

# The first line of each pair from here down creates subset tables based on what is in the "code.subject" data column. The second line of each pair extracts just the TA comments so they can be tokenized.
base_data_3columns.basic <- subset(base_data, code.subject == "1. Basic Criteria", select = colnames.base_data)
base_data_comments.basic <- as.data.frame(base_data_3columns.basic[,2], drop=FALSE, stringsAsFactors=FALSE) #assumes Col. 2 is the ta.commment column

base_data_3columns.writing <- subset(base_data, code.subject == "2. Writing Quality", select = colnames.base_data)
base_data_comments.writing <- as.data.frame(base_data_3columns.writing[,2], drop=FALSE, stringsAsFactors=FALSE) #assumes Col. 2 is the ta.commment column

base_data_3columns.technical <- subset(base_data, code.subject == "3. Technical and Scientific", select = colnames.base_data)
base_data_comments.technical <- as.data.frame(base_data_3columns.technical[,2], drop=FALSE, stringsAsFactors=FALSE) #assumes Col. 2 is the ta.commment column

base_data_3columns.logic <- subset(base_data, code.subject == "4. Logic and Thinking", select = colnames.base_data)
base_data_comments.logic <- as.data.frame(base_data_3columns.logic[,2], drop=FALSE, stringsAsFactors=FALSE) #assumes Col. 2 is the ta.commment column

base_data_3columns.praise <- subset(base_data, code.subject == "5. Praise or Concern", select = colnames.base_data)
base_data_comments.praise <- as.data.frame(base_data_3columns.praise[,2], drop=FALSE, stringsAsFactors=FALSE) #assumes Col. 2 is the ta.commment column

base_data_3columns.misconduct <- subset(base_data, code.subject == "6. Misconduct", select = colnames.base_data)
base_data_comments.misconduct <- as.data.frame(base_data_3columns.misconduct[,2], drop=FALSE, stringsAsFactors=FALSE) #assumes Col. 2 is the ta.commment column

base_data_3columns.narrative <- subset(base_data, code.subject == "12. Narrative Comments", select = colnames.base_data)
base_data_comments.narrative <- as.data.frame(base_data_3columns.narrative[,2], drop=FALSE, stringsAsFactors=FALSE) #assumes Col. 2 is the ta.commment column

base_data_3columns.sciname <- subset(base_data, code.subject == "Scientific Name", select =  colnames.base_data)
base_data_comments.sciname <- as.data.frame(base_data_3columns.sciname[,2], drop=FALSE, stringsAsFactors=FALSE) #assumes Col. 2 is the ta.commment column
```

This second series of commands creates tokenized words lists for all comments together, and token subsets for each comments subset.
```{r}
base_data_comments.all.tokens <- base_data_comments %>% 
  unnest_tokens(word,ta.comment)
base_data_comments.basic.tokens <- base_data_comments.basic %>% 
  unnest_tokens(word,ta.comment)
base_data_comments.writing.tokens <- base_data_comments.writing %>% 
  unnest_tokens(word,ta.comment)
base_data_comments.technical.tokens <- base_data_comments.technical %>% 
  unnest_tokens(word,ta.comment)
base_data_comments.logic.tokens <- base_data_comments.logic %>% 
  unnest_tokens(word,ta.comment)
base_data_comments.praise.tokens <- base_data_comments.praise %>% 
  unnest_tokens(word,ta.comment)
base_data_comments.misconduct.tokens <- base_data_comments.misconduct %>% 
  unnest_tokens(word,ta.comment)
base_data_comments.narrative.tokens <- base_data_comments.narrative %>% 
  unnest_tokens(word,ta.comment)
base_data_comments.sciname.tokens <- base_data_comments.sciname %>% 
  unnest_tokens(word,ta.comment)

#Display the last set of tokens to show process finished.
base_data_comments.sciname.tokens
```


***

# Tokenizing Text of Comments
Create a single-column data frame of just the comments for ALL lines. Then check that the data was written to the file as characters not factors.



####If you must convert data to a Tibble:
To make tokenizing work you must convert the desired set of extracted data into a tibble. You HAVE to use a tibble to tokenize text lines. Trying to do it straight from a values table will throw an error.

```
base_data_comments_50_tibble <- tibble(line = seq_along(data), text = base_data_comments_50(c[,2])) #tibble-izer

#base_data_comments_50_tibble %>% 
#  unnest_tokens(word,ta.comment)
```
unique.record, code.subject, code.structure
base_data


***
***

##FWIW
This command uses a tibble with 3 data columns, and successfully unnests the tokens. It is messy because the other columns are duplicated, BUT at least it is possible to wrangle a multi-column tibble for tokenization.
```{r}
Abase_data_3columns.sciname.tokens <- base_data_3columns.sciname %>% 
  unnest_tokens(word,ta.comment)
```



***
#Approach ONE: Work backwards from existing sorting to get criteria.
I have a pre-defined set of traits. Can I build from that?

Do it using naive Bayes. 

##Step 1.1: Regenerate the groups in two-dimensional array


##Step 1.2: Yada


##Step 1.3: Yada


##Step 1.4: Yada


***
#Approach TWO: Assume no a priori groups, and look for natural clusters. 



##Step 2.1: Yada


##Step 2.2: Yada


##Step 2.3: Yada


##Step 2.4: Yada

