sms_train_labels2 <- sms_raw2[1:5990, ]$type
sms_test_labels2 <- sms_raw2[5991:7987,]$type
prop.table(table(sms_train_labels2))
prop.table(table(sms_test_labels2))
wordcloud(sms_corpus_clean2, max.words = 50, random.order = FALSE)
TWO <- subset(sms_raw2, type == "2_Writing")
THREE <- subset(sms_raw2, type == "3_Technical")
#FOUR <- subset(sms_raw2, type == "4_Logic")
wordcloud(TWO$text, max.words = 50, scale = c(3, 0.5))
wordcloud(THREE$text, max.words = 50, scale = c(3, 0.5))
#wordcloud(FOUR$text, max.words = 50, scale = c(3, 0.5))
sms_freq_words2 <- findFreqTerms(sms_dtm_train2, 5)
str(sms_freq_words2)
sms_dtm_freq_train2 <- sms_dtm_train2[ , sms_freq_words2]
sms_dtm_freq_test2 <- sms_dtm_test2[ , sms_freq_words2]
convert_counts2 <- function(x) {
x <- ifelse(x > 0, "Yes", "No")
}
sms_train2 <- apply(sms_dtm_freq_train2, MARGIN = 2, convert_counts2)
sms_test2 <- apply(sms_dtm_freq_test2, MARGIN = 2, convert_counts2)
sms_classifier2 <- naiveBayes(sms_train2, sms_train_labels2)
sms_test_pred2 <- predict(sms_classifier2, sms_test2)
CrossTable(sms_test_pred2, sms_test_labels2, prop.chisq = FALSE, prop.t = FALSE, dnn = c('predicted', 'actual'))
sack2 <- TermDocumentMatrix(sms_corpus_clean2)
pack2 <- as.matrix(sack2)
snack2 <- sort(rowSums(pack2), decreasing = TRUE)
hack2 <- data.frame(word = names(snack2), freq=snack2)
head(hack2, 10)
wordcloud(TWO$text, max.words = 10, scale = c(3, 0.5))
wordcloud(THREE$text, max.words = 10, scale = c(3, 0.5))
#wordcloud(FOUR$text, max.words = 10, scale = c(3, 0.5))
#Read in CSV file named "coded_full_comments_dataset_Spring18anon.csv".
base_data <- read_csv(file='data/coded_full_comments_dataset_Spring18anon.csv')
#Isolate rows to compare.
frequency_writing <- filter(base_data,code.subject=="2. Writing Quality"|code.subject=="4. Logic and Thinking") #|code.subject=="3. Technical and Scientific"
#Reduce larger dataframe to 2 required columns of data, and put columns in order needed.
sms_raw2 <- frequency_writing %>% select(23,22)
#Rename the columns.
names(sms_raw2)[1] <- "type"
names(sms_raw2)[2] <- "text"
#Simplify coding terms
sms_raw2[,1] <- ifelse(sms_raw2[,1] == "2. Writing Quality", "2_Writing", ifelse(sms_raw2[,1] == "4. Logic and Thinking","4_Logic", 99))# ifelse(sms_raw2[,1] == ,99)))"3. Technical and Scientific", "3_Technical"
#Change "type" element from character to a factor for analysis.
sms_raw2$type <- factor(sms_raw2$type)
str(sms_raw2$type)
table(sms_raw2$type)
#Read in CSV file named "coded_full_comments_dataset_Spring18anon.csv".
base_data <- read_csv(file='data/coded_full_comments_dataset_Spring18anon.csv')
#Isolate rows to compare.
frequency_writing <- filter(base_data,code.subject=="2. Writing Quality"|code.subject=="4. Logic and Thinking") #|code.subject=="3. Technical and Scientific"
#Reduce larger dataframe to 2 required columns of data, and put columns in order needed.
sms_raw2 <- frequency_writing %>% select(23,22)
#Rename the columns.
names(sms_raw2)[1] <- "type"
names(sms_raw2)[2] <- "text"
#Simplify coding terms
sms_raw2[,1] <- ifelse(sms_raw2[,1] == "2. Writing Quality", "2_Writing", ifelse(sms_raw2[,1] == "4. Logic and Thinking","4_Logic", 99))# ifelse(sms_raw2[,1] == ,99)))"3. Technical and Scientific", "3_Technical"
#Change "type" element from character to a factor for analysis.
sms_raw2$type <- factor(sms_raw2$type)
str(sms_raw2$type)
table(sms_raw2$type)
##Text Transformation using "tm" package in R to create a volitile coprus that contains the "text" vector from our data frame.
sms_corpus2 <- VCorpus(VectorSource(sms_raw2$text))
print(sms_corpus2)
#Check out the first few messages in the new corpus, which is basically a list that can be manipulated with list operations.
inspect(sms_corpus2[1:3])
#Use "as.character" function to see what a message looks like.
as.character(sms_corpus2[[3]])
sms_corpus_clean2 <- tm_map(sms_corpus2, content_transformer(tolower))
as.character(sms_corpus2[[3]])
as.character((sms_corpus_clean2[[3]]))
sms_corpus_clean2 <- tm_map(sms_corpus_clean2, removeNumbers)
sms_corpus_clean2 <- tm_map(sms_corpus_clean2, removeWords, stopwords())
sms_corpus_clean2 <- tm_map(sms_corpus_clean2, removePunctuation)
as.character((sms_corpus_clean2[[3]]))
sms_corpus_clean2 <- tm_map(sms_corpus_clean2, stripWhitespace)
as.character(sms_corpus_clean2[[3]])
sms_dtm2 <- DocumentTermMatrix(sms_corpus_clean2)
.75 * 3720
.25 * 9129
sms_dtm_train2 <- sms_dtm2[1:2790, ]
sms_dtm_test2 <- sms_dtm2[2791:3720, ]
sms_train_labels2 <- sms_raw2[1:2790, ]$type
sms_test_labels2 <- sms_raw2[2791:3720,]$type
prop.table(table(sms_train_labels2))
prop.table(table(sms_test_labels2))
wordcloud(sms_corpus_clean2, max.words = 50, random.order = FALSE)
TWO <- subset(sms_raw2, type == "2_Writing")
#THREE <- subset(sms_raw2, type == "3_Technical")
FOUR <- subset(sms_raw2, type == "4_Logic")
wordcloud(TWO$text, max.words = 50, scale = c(3, 0.5))
#wordcloud(THREE$text, max.words = 50, scale = c(3, 0.5))
wordcloud(FOUR$text, max.words = 50, scale = c(3, 0.5))
sms_freq_words2 <- findFreqTerms(sms_dtm_train2, 5)
str(sms_freq_words2)
sms_dtm_freq_train2 <- sms_dtm_train2[ , sms_freq_words2]
sms_dtm_freq_test2 <- sms_dtm_test2[ , sms_freq_words2]
convert_counts2 <- function(x) {
x <- ifelse(x > 0, "Yes", "No")
}
sms_train2 <- apply(sms_dtm_freq_train2, MARGIN = 2, convert_counts2)
sms_test2 <- apply(sms_dtm_freq_test2, MARGIN = 2, convert_counts2)
sms_classifier2 <- naiveBayes(sms_train2, sms_train_labels2)
sms_test_pred2 <- predict(sms_classifier2, sms_test2)
CrossTable(sms_test_pred2, sms_test_labels2, prop.chisq = FALSE, prop.t = FALSE, dnn = c('predicted', 'actual'))
#Read in CSV file named "coded_full_comments_dataset_Spring18anon.csv".
base_data <- read_csv(file='data/coded_full_comments_dataset_Spring18anon.csv')
#Isolate rows to compare.
frequency_writing <- filter(base_data,code.subject=="3. Technical and Scientific"|code.subject=="4. Logic and Thinking") #|code.subject=="2. Writing Quality"
#Reduce larger dataframe to 2 required columns of data, and put columns in order needed.
sms_raw2 <- frequency_writing %>% select(23,22)
#Rename the columns.
names(sms_raw2)[1] <- "type"
names(sms_raw2)[2] <- "text"
#Simplify coding terms
sms_raw2[,1] <- ifelse(sms_raw2[,1] == "3. Technical and Scientific", "3_Technical", ifelse(sms_raw2[,1] == "4. Logic and Thinking","4_Logic", 99))# ifelse(sms_raw2[,1] == ,99)))"2. Writing Quality", "2_Writing"
#Change "type" element from character to a factor for analysis.
sms_raw2$type <- factor(sms_raw2$type)
str(sms_raw2$type)
table(sms_raw2$type)
##Text Transformation using "tm" package in R to create a volitile coprus that contains the "text" vector from our data frame.
sms_corpus2 <- VCorpus(VectorSource(sms_raw2$text))
print(sms_corpus2)
#Check out the first few messages in the new corpus, which is basically a list that can be manipulated with list operations.
inspect(sms_corpus2[1:3])
#Use "as.character" function to see what a message looks like.
as.character(sms_corpus2[[3]])
sms_corpus_clean2 <- tm_map(sms_corpus2, content_transformer(tolower))
as.character(sms_corpus2[[3]])
as.character((sms_corpus_clean2[[3]]))
sms_corpus_clean2 <- tm_map(sms_corpus_clean2, removeNumbers)
sms_corpus_clean2 <- tm_map(sms_corpus_clean2, removeWords, stopwords())
sms_corpus_clean2 <- tm_map(sms_corpus_clean2, removePunctuation)
as.character((sms_corpus_clean2[[3]]))
sms_corpus_clean2 <- tm_map(sms_corpus_clean2, stripWhitespace)
as.character(sms_corpus_clean2[[3]])
sms_dtm2 <- DocumentTermMatrix(sms_corpus_clean2)
.75 * 6551
.25 * 9129
sms_dtm_train2 <- sms_dtm2[1:4913, ]
sms_dtm_test2 <- sms_dtm2[4914:6551, ]
sms_train_labels2 <- sms_raw2[1:4913, ]$type
sms_test_labels2 <- sms_raw2[4914:6551,]$type
prop.table(table(sms_train_labels2))
prop.table(table(sms_test_labels2))
wordcloud(sms_corpus_clean2, max.words = 50, random.order = FALSE)
#TWO <- subset(sms_raw2, type == "2_Writing")
THREE <- subset(sms_raw2, type == "3_Technical")
FOUR <- subset(sms_raw2, type == "4_Logic")
#wordcloud(TWO$text, max.words = 50, scale = c(3, 0.5))
wordcloud(THREE$text, max.words = 50, scale = c(3, 0.5))
wordcloud(FOUR$text, max.words = 50, scale = c(3, 0.5))
sms_freq_words2 <- findFreqTerms(sms_dtm_train2, 5)
str(sms_freq_words2)
sms_dtm_freq_train2 <- sms_dtm_train2[ , sms_freq_words2]
sms_dtm_freq_test2 <- sms_dtm_test2[ , sms_freq_words2]
convert_counts2 <- function(x) {
x <- ifelse(x > 0, "Yes", "No")
}
sms_train2 <- apply(sms_dtm_freq_train2, MARGIN = 2, convert_counts2)
sms_test2 <- apply(sms_dtm_freq_test2, MARGIN = 2, convert_counts2)
sms_classifier2 <- naiveBayes(sms_train2, sms_train_labels2)
sms_test_pred2 <- predict(sms_classifier2, sms_test2)
CrossTable(sms_test_pred2, sms_test_labels2, prop.chisq = FALSE, prop.t = FALSE, dnn = c('predicted', 'actual'))
View(THREE)
View(sms_test2)
knitr::opts_chunk$set(echo = TRUE)
sms_corpus_clean2 <- tm_map(sms_corpus_clean2, removePunctuation)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidytext)
library(dplyr)
library(tidyr)
library(ggplot2)
library(scales)
library(tm)
library(SnowballC)
library(wordcloud)
library(e1071)
library(gmodels)
library(readr)
#Read in CSV file named "coded_full_comments_dataset_Spring18anon.csv".
base_data <- read_csv(file='data/coded_full_comments_dataset_Spring18anon.csv')
#Isolate rows to compare.
frequency_writing <- filter(base_data,code.subject=="2. Writing Quality"|code.subject=="3. Technical and Scientific"|code.subject=="4. Logic and Thinking")
#Reduce larger dataframe to 2 required columns of data, and put columns in order needed.
sms_raw2 <- frequency_writing %>% select(23,22)
#Rename the columns.
names(sms_raw2)[1] <- "type"
names(sms_raw2)[2] <- "text"
#Simplify coding terms
sms_raw2[,1] <- ifelse(sms_raw2[,1] == "2. Writing Quality", "2_Writing", ifelse(sms_raw2[,1] == "3. Technical and Scientific", "3_Technical", ifelse(sms_raw2[,1] == "4. Logic and Thinking","4_Logic",99)))
#Change "type" element from character to a factor for analysis.
sms_raw2$type <- factor(sms_raw2$type)
str(sms_raw2$type)
table(sms_raw2$type)
##Text Transformation using "tm" package in R to create a volitile coprus that contains the "text" vector from our data frame.
sms_corpus2 <- VCorpus(VectorSource(sms_raw2$text))
print(sms_corpus2)
#Check out the first few messages in the new corpus, which is basically a list that can be manipulated with list operations.
inspect(sms_corpus2[1:3])
#Use "as.character" function to see what a message looks like.
as.character(sms_corpus2[[3]])
sms_corpus_clean2 <- tm_map(sms_corpus2, content_transformer(tolower))
as.character(sms_corpus2[[3]])
as.character((sms_corpus_clean2[[3]]))
sms_corpus_clean2 <- tm_map(sms_corpus_clean2, removeNumbers)
sms_corpus_clean2 <- tm_map(sms_corpus_clean2, removeWords, stopwords())
sms_corpus_clean2 <- tm_map(sms_corpus_clean2, removePunctuation)
as.character((sms_corpus_clean2[[3]]))
sms_corpus_clean2 <- tm_map(sms_corpus_clean2, stripWhitespace)
as.character(sms_corpus_clean2[[3]])
sms_dtm2 <- DocumentTermMatrix(sms_corpus_clean2)
View(sms_dtm2)
sms_dtm2
sms_dtm2 <- DocumentTermMatrix(sms_corpus_clean2)
sms_dtm2_tidy <- tidy(sms_dtm2)
View(sms_dtm2_tidy)
View(sms_corpus_clean2)
sms_dtm_bigram <- DocumentTermMatrix(sms_corpus_clean2, control = list(tokenize=TrigramTokenizer))
sms_dtm_bigram <- DocumentTermMatrix(sms_corpus_clean2, control = list(tokenize=trigram))
View(sms_raw2)
View(sms_dtm2)
View(sms_raw2)
library(corpus)
install.packages("corpus")
library(corpus)
corpus <- sms_raw2$text
text_filter(corpus)$drop_punct <- TRUE # ignore punctuation
library(corpus)
corpus <- sms_raw2$text
text_filter(corpus)$drop_punct=TRUE # ignore punctuation
library(corpus)
corpus <- sms_raw2$text
text_filter(corpus)$drop_punct="TRUE"
library(corpus)
corpus <- sms_raw2$text
text_filter(corpus)$drop_punct==TRUE
term_stats(corpus, ngrams = 2:3)
library(corpus)
corpus <- sms_raw2$text
text_filter(corpus)$drop_punct==FALSE
term_stats(corpus, ngrams = 2:3)
library(corpus)
corpus <- sms_raw2$text
corpus <- tm_map(corpus, removePunctuation)
library(corpus)
corpus <- sms_raw2$text
corpus2 <- tm_map(corpus, removePunctuation)
##Text Transformation using "tm" package in R to create a volitile coprus that contains the "text" vector from our data frame.
sms_corpus2 <- VCorpus(VectorSource(sms_raw2$text))
print(sms_corpus2)
#Check out the first few messages in the new corpus, which is basically a list that can be manipulated with list operations.
inspect(sms_corpus2[1:3])
#Use "as.character" function to see what a message looks like.
as.character(sms_corpus2[[3]])
sms_corpus_clean2 <- tm_map(sms_corpus2, content_transformer(tolower))
as.character(sms_corpus2[[3]])
as.character((sms_corpus_clean2[[3]]))
sms_corpus_clean2 <- tm_map(sms_corpus_clean2, removeNumbers)
sms_corpus_clean2 <- tm_map(sms_corpus_clean2, removePunctuation)
as.character((sms_corpus_clean2[[3]]))
as.character(sms_corpus_clean2[[3]])
library(corpus)
corpus <- sms_corpus_clean2
text_filter(corpus)$drop_punct==FALSE
term_stats(corpus, ngrams = 2:3)
sms_dtm2 <- DocumentTermMatrix(sms_corpus_clean2)
sms_dtm2_tidy <- tidy(sms_dtm2)
library(corpus)
corpus <- sms_corpus_clean2
text_filter(corpus)$drop_punct==FALSE
term_stats(corpus, ngrams = 2:3)
sms_dtm_bigram <- DocumentTermMatrix(sms_corpus_clean2, control = list(term_stats))
View(sms_dtm_bigram)
library(corpus)
corpus <- sms_corpus_clean2
text_filter(corpus)$drop_punct==FALSE
term_stats(corpus, ngrams = 2:3)
sms_dtm_bigram <- DocumentTermMatrix(sms_corpus_clean2, control = list(term_stats))
sms_dtm2_bigram_tidy <- tidy(sms_dtm2_bigram)
library(corpus)
corpus <- sms_corpus_clean2
text_filter(corpus)$drop_punct==FALSE
term_stats(corpus, ngrams = 2:3)
sms_dtm_bigram <- DocumentTermMatrix(sms_corpus_clean2, control = list(term_stats))
sms_dtm_bigram_tidy <- tidy(sms_dtm_bigram)
View(sms_dtm_bigram_tidy)
sms_dtm2 <- DocumentTermMatrix(sms_corpus_clean2)
sms_dtm2_tidy <- tidy(sms_dtm2)
View(sms_dtm2)
View(sms_dtm2_tidy)
View(sms_dtm_bigram_tidy)
library(corpus)
corpus <- sms_corpus_clean2
text_filter(corpus)$drop_punct==TRUE
term_stats(corpus, ngrams = 2:3)
library(corpus)
corpus <- sms_corpus_clean2
text_filter(corpus)$drop_punct==TRUE
term_stats(corpus, ngrams = 2:3)
sms_dtm_bigram <- DocumentTermMatrix(sms_corpus_clean2, control = list(tokenize=term_stats))
twitterTdm <- TermDocumentMatrix(sms_corpus_clean2["twitter"],control=list(tokenize=TrigramTokenizer))
install.packages("RWeka")
library(RWeka)
NLPtrigramTokenizer <- function(x) {
unlist(lapply(ngrams(words(x), 3), paste, collapse = " "), use.names = FALSE)
}
tdm_NLP <- TermDocumentMatrix(sms_corpus_clean2, control=list(tokenize = NLPtrigramTokenizer))
inspect(tdm_NLP)
View(tdm_NLP)
sms_dtm_trigram <- DocumentTermMatrix(sms_corpus_clean2, control = list(tokenize=tdm_NLP))
tdm_NLP_tidy <- tidy(tdm_NLP)
#sms_dtm_trigram <- DocumentTermMatrix(sms_corpus_clean2, control = list(tokenize=tdm_NLP))
#sms_dtm_trigram_tidy <- tidy(sms_dtm_trigram)
View(tdm_NLP_tidy)
NLPtrigramTokenizer <- function(x) {
unlist(lapply(ngrams(words(x), 3), paste, collapse = " "), use.names = FALSE)
}
sms_dtm2_trigrams <- TermDocumentMatrix(sms_corpus_clean2, control=list(tokenize = NLPtrigramTokenizer))
#Read in CSV file named "coded_full_comments_dataset_Spring18anon.csv".
base_data <- read_csv(file='data/coded_full_comments_dataset_Spring18anon.csv')
#Isolate rows to compare.
frequency_writing <- filter(base_data,code.subject=="2. Writing Quality"|code.subject=="3. Technical and Scientific"|code.subject=="4. Logic and Thinking")
#Reduce larger dataframe to 2 required columns of data, and put columns in order needed.
sms_raw2 <- frequency_writing %>% select(23,22)
#Rename the columns.
names(sms_raw2)[1] <- "type"
names(sms_raw2)[2] <- "text"
#Simplify coding terms
sms_raw2[,1] <- ifelse(sms_raw2[,1] == "2. Writing Quality", "2_Writing", ifelse(sms_raw2[,1] == "3. Technical and Scientific", "3_Technical", ifelse(sms_raw2[,1] == "4. Logic and Thinking","4_Logic",99)))
#Change "type" element from character to a factor for analysis.
sms_raw2$type <- factor(sms_raw2$type)
str(sms_raw2$type)
table(sms_raw2$type)
##Text Transformation using "tm" package in R to create a volitile coprus that contains the "text" vector from our data frame.
sms_corpus2 <- VCorpus(VectorSource(sms_raw2$text))
#Convert to all lower case letters with "tm_map"" funtion in R, then use the "content_transformer" function to transform the text.
sms_corpus_clean2 <- tm_map(sms_corpus2, content_transformer(tolower))
#This removes numbers. **May not want to do.**
sms_corpus_clean2 <- tm_map(sms_corpus_clean2, removeNumbers)
#Stopword removal. **One of variables to test out.**
sms_corpus_clean2 <- tm_map(sms_corpus_clean2, removeWords, stopwords())
#Remove punctuation as well using the "removePunctuation" function. Try not doing this. Removes evidence of questions.
sms_corpus_clean2 <- tm_map(sms_corpus_clean2, removePunctuation)
sms_corpus_clean2 <- tm_map(sms_corpus_clean2, stemDocument)
sms_corpus_clean2 <- tm_map(sms_corpus_clean2, stripWhitespace)
as.character(sms_corpus_clean2[[3]])
##Text Transformation using "tm" package in R to create a volitile coprus that contains the "text" vector from our data frame.
sms_corpus2 <- VCorpus(VectorSource(sms_raw2$text))
#Convert to all lower case letters with "tm_map"" funtion in R, then use the "content_transformer" function to transform the text.
sms_corpus_clean2 <- tm_map(sms_corpus2, content_transformer(tolower))
#This removes numbers. **May not want to do.**
sms_corpus_clean2 <- tm_map(sms_corpus_clean2, removeNumbers)
#Stopword removal. **One of variables to test out.**
sms_corpus_clean2 <- tm_map(sms_corpus_clean2, removeWords, stopwords())
#Remove punctuation as well using the "removePunctuation" function. Try not doing this. Removes evidence of questions.
sms_corpus_clean2 <- tm_map(sms_corpus_clean2, removePunctuation)
sms_corpus_clean2 <- tm_map(sms_corpus_clean2, stripWhitespace)
as.character(sms_corpus_clean2[[3]])
NLPtrigramTokenizer <- function(x) {
unlist(lapply(ngrams(words(x), 3), paste, collapse = " "), use.names = FALSE)
}
sms_dtm2_trigrams <- TermDocumentMatrix(sms_corpus_clean2, control=list(tokenize = NLPtrigramTokenizer))
inspect(sms_dtm2_trigrams)
sms_dtm2_trigrams_tidy <- tidy(sms_dtm2_trigrams)
#sms_dtm_trigram <- DocumentTermMatrix(sms_corpus_clean2, control = list(tokenize=tdm_NLP))
#sms_dtm_trigram_tidy <- tidy(sms_dtm_trigram)
View(sms_dtm2_trigrams_tidy)
NLP_Tokenizer <- function(x) {
unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)
}
sms_dtm2_trigrams <- TermDocumentMatrix(sms_corpus_clean2, control=list(tokenize = NLP_Tokenizer))
inspect(sms_dtm2_trigrams)
sms_dtm2 <- DocumentTermMatrix(sms_corpus_clean2)
sms_dtm2_tidy <- tidy(sms_dtm2)
sms_dtm2_bigrams_tidy <- tidy(sms_dtm2_bigrams)
NLP_Tokenizer <- function(x) {
unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)
}
sms_dtm2_bigrams <- TermDocumentMatrix(sms_corpus_clean2, control=list(tokenize = NLP_Tokenizer))
inspect(sms_dtm2_bigrams)
sms_dtm2_bigrams_tidy <- tidy(sms_dtm2_bigrams)
#sms_dtm_trigram <- DocumentTermMatrix(sms_corpus_clean2, control = list(tokenize=tdm_NLP))
#sms_dtm_trigram_tidy <- tidy(sms_dtm_trigram)
View(sms_dtm2_bigrams_tidy)
View(sms_raw2)
.75 * 56741
.25 * 9129
.75 * 56741
.25 * 56741
sms_dtm_train2 <- sms_dtm2_bigrams[1:42555, ]
View(sms_dtm2_bigrams_tidy)
.75 * 56741
.25 * 56741
sms_dtm_train2 <- sms_dtm2_bigrams[1:42554, ]
View(sms_dtm2_bigrams)
.75 * 9129
.25 * 9129
sms_dtm_train2 <- sms_dtm2_bigrams[1:6846, ]
sms_dtm_test2 <- sms_dtm2_bigrams[6847:9129, ]
sms_train_labels2 <- sms_raw2[1:6846, ]$type
sms_test_labels2 <- sms_raw2[6847:9129,]$type
prop.table(table(sms_train_labels2))
prop.table(table(sms_test_labels2))
wordcloud(sms_corpus_clean2, max.words = 50, random.order = FALSE)
TWO <- subset(sms_raw2, type == "2_Writing")
THREE <- subset(sms_raw2, type == "3_Technical")
FOUR <- subset(sms_raw2, type == "4_Logic")
wordcloud(TWO$text, max.words = 50, scale = c(3, 0.5))
wordcloud(THREE$text, max.words = 50, scale = c(3, 0.5))
wordcloud(FOUR$text, max.words = 50, scale = c(3, 0.5))
sms_freq_words2 <- findFreqTerms(sms_dtm_train2, 5)
str(sms_freq_words2)
sms_dtm_freq_train2 <- sms_dtm_train2[ , sms_freq_words2]
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidytext)
library(dplyr)
library(tidyr)
library(ggplot2)
library(scales)
library(tm)
library(SnowballC)
library(wordcloud)
library(e1071)
library(gmodels)
library(readr)
#Read in CSV file named "coded_full_comments_dataset_Spring18anon.csv".
base_data <- read_csv(file='data/coded_full_comments_dataset_Spring18anon.csv')
#Isolate rows to compare.
frequency_writing <- filter(base_data,code.subject=="2. Writing Quality"|code.subject=="3. Technical and Scientific"|code.subject=="4. Logic and Thinking")
#Reduce larger dataframe to 2 required columns of data, and put columns in order needed.
sms_raw2 <- frequency_writing %>% select(23,22)
#Rename the columns.
names(sms_raw2)[1] <- "type"
names(sms_raw2)[2] <- "text"
#Simplify coding terms
sms_raw2[,1] <- ifelse(sms_raw2[,1] == "2. Writing Quality", "2_Writing", ifelse(sms_raw2[,1] == "3. Technical and Scientific", "3_Technical", ifelse(sms_raw2[,1] == "4. Logic and Thinking","4_Logic",99)))
#Change "type" element from character to a factor for analysis.
sms_raw2$type <- factor(sms_raw2$type)
str(sms_raw2$type)
table(sms_raw2$type)
##Text Transformation using "tm" package in R to create a volitile coprus that contains the "text" vector from our data frame.
sms_corpus2 <- VCorpus(VectorSource(sms_raw2$text))
#Convert to all lower case letters with "tm_map"" funtion in R, then use the "content_transformer" function to transform the text.
sms_corpus_clean2 <- tm_map(sms_corpus2, content_transformer(tolower))
#This removes numbers. **May not want to do.**
sms_corpus_clean2 <- tm_map(sms_corpus_clean2, removeNumbers)
#Stopword removal. **One of variables to test out.**
sms_corpus_clean2 <- tm_map(sms_corpus_clean2, removeWords, stopwords())
#Remove punctuation as well using the "removePunctuation" function. Try not doing this. Removes evidence of questions.
sms_corpus_clean2 <- tm_map(sms_corpus_clean2, removePunctuation)
View(sms_corpus_clean2)
sms_corpus_clean2 <- tm_map(sms_corpus_clean2, stripWhitespace)
as.character(sms_corpus_clean2[[3]])
NLP_Tokenizer <- function(x) {
unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)
}
sms_dtm2_bigrams <- TermDocumentMatrix(sms_corpus_clean2, control=list(tokenize = NLP_Tokenizer))
inspect(sms_dtm2_bigrams)
sms_dtm2_bigrams_tidy <- tidy(sms_dtm2_bigrams)
#sms_dtm_trigram <- DocumentTermMatrix(sms_corpus_clean2, control = list(tokenize=tdm_NLP))
#sms_dtm_trigram_tidy <- tidy(sms_dtm_trigram)
View(sms_dtm2_bigrams_tidy)
sms_dtm_train2 <- sms_dtm2_bigrams[1:6846, ]
sms_dtm_test2 <- sms_dtm2_bigrams[6847:9129, ]
sms_train_labels2 <- sms_raw2[1:6846, ]$type
sms_test_labels2 <- sms_raw2[6847:9129,]$type
prop.table(table(sms_train_labels2))
prop.table(table(sms_test_labels2))
sms_freq_words2 <- findFreqTerms(sms_dtm_train2, 5)
str(sms_freq_words2)
sms_dtm_freq_train2 <- sms_dtm_train2[ , sms_freq_words2]
wordcloud(sms_corpus_clean2, max.words = 50, random.order = FALSE)
TWO <- subset(sms_raw2, type == "2_Writing")
THREE <- subset(sms_raw2, type == "3_Technical")
FOUR <- subset(sms_raw2, type == "4_Logic")
wordcloud(TWO$text, max.words = 50, scale = c(3, 0.5))
wordcloud(THREE$text, max.words = 50, scale = c(3, 0.5))
wordcloud(FOUR$text, max.words = 50, scale = c(3, 0.5))
sms_freq_words2 <- findFreqTerms(sms_dtm_train2, 5)
str(sms_freq_words2)
sms_dtm_freq_train2 <- sms_dtm_train2[ , sms_freq_words2]
setwd("~/Dropbox/Coding_Tools/R_Environment/R_Projects/ta_comments_FLC")
sms_dtm_train2 <- sms_dtm2[1:6846, ]
sms_dtm2 <- DocumentTermMatrix(sms_corpus_clean2)
sms_dtm2_tidy <- tidy(sms_dtm2)
sms_dtm_train2 <- sms_dtm2[1:6846, ]
sms_dtm_test2 <- sms_dtm2[6847:9129, ]
sms_dtm_train2 <- sms_dtm2_bigrams[1:6846, ]
sms_dtm_test2 <- sms_dtm2_bigrams[6847:9129, ]
sms_train_labels2 <- sms_raw2[1:6846, ]$type
sms_test_labels2 <- sms_raw2[6847:9129,]$type
prop.table(table(sms_train_labels2))
prop.table(table(sms_test_labels2))
sms_freq_words2 <- findFreqTerms(sms_dtm_train2, 5)
str(sms_freq_words2)
sms_dtm_freq_train2 <- sms_dtm_train2[ , sms_freq_words2]
sms_dtm2 <- DocumentTermMatrix(sms_corpus_clean2)
sms_dtm2_tidy <- tidy(sms_dtm2)
View(sms_dtm2_tidy)
NLP_Tokenizer <- function(x) {
unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)
}
sms_dtm2_bigrams <- TermDocumentMatrix(sms_corpus_clean2, control=list(tokenize = NLP_Tokenizer))
inspect(sms_dtm2_bigrams)
sms_dtm2_bigrams_tidy <- tidy(sms_dtm2_bigrams)
#sms_dtm_trigram <- DocumentTermMatrix(sms_corpus_clean2, control = list(tokenize=tdm_NLP))
#sms_dtm_trigram_tidy <- tidy(sms_dtm_trigram)
View(sms_dtm2_bigrams_tidy)
View(sms_corpus_clean2)
