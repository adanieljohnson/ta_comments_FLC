---
title: "Naive Bayes Workflow"
author: "Dan Johnson"
date: "12/21/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
##Summary
This notebook documents a basic workflow for applying a naive Bayes model to terms in TA comments. Rows ~30-200 are the initial version that I based on this tutorial [https://rpubs.com/Billyhansen6/318412]. Various permutation tests start around row #210.


##Initial Setup
```{r}
library(tidyverse)
library(tidytext)
library(tidyr)
library(tm)
library(SnowballC)
library(wordcloud)
library(e1071)
library(gmodels)
```

####\  

##Baseline Version 1.0 of Naive Bayes (NB) Protocol
Initial data input.
```{r}
#Read in TA comments from CSV file.
base_data <- read_csv(file='data/coded_full_comments_dataset_Spring18anon.csv')

#Select rows representing the sub-groups to compare. 
comments_subset <- filter(base_data,code.subject=="1. Basic Criteria"|code.subject=="2. Writing Quality"|code.subject=="3. Technical and Scientific"|code.subject=="4. Logic and Thinking")

#Reduce larger dataframe to 2 required columns of data, and put columns in order needed.
comments_raw <- comments_subset %>% select(23,22)

#Rename the columns.
names(comments_raw)[1] <- "type"
names(comments_raw)[2] <- "text"

#Simplify coding terms
comments_raw[,1] <- ifelse(comments_raw[,1] == "1. Basic Criteria","1_Basic", ifelse(comments_raw[,1] == "2. Writing Quality","2_Writing", ifelse(comments_raw[,1] == "3. Technical and Scientific","3_Technical", ifelse(comments_raw[,1] == "4. Logic and Thinking","4_Logic",99))))

#Change "type" element from character to a factor for analysis.
comments_raw$type <- factor(comments_raw$type)
str(comments_raw$type)
table(comments_raw$type)
```

Data set must be converted to a volative corpus using "tm" library then transformed.
```{r}
#Create the volatile coprus that contains the "text" vector from data frame.
comments_corpus <- VCorpus(VectorSource(comments_raw$text))
print(comments_corpus)

#Check out the first few messages in the new corpus, which is basically a list that can be manipulated with list operations.
inspect(comments_corpus[1:3])

#Use "as.character" function to see what a message looks like.
as.character(comments_corpus[[3]])
```

These are the OPTIONAL data transforms using "tm_map"" and "content_transformer" functions.
```{r}
#Convert to all lower case letters.
comments_corpus_clean <- tm_map(comments_corpus, content_transformer(tolower))
```

```{r}
#Remove numbers. 
comments_corpus_clean <- tm_map(comments_corpus_clean, removeNumbers)
```

```{r}
#Stopword removal. 
comments_corpus_clean <- tm_map(comments_corpus_clean, removeWords, stopwords())
```

```{r}
#Remove punctuation as well using the "removePunctuation" function. 
#This step removes evidence of questions, so may remove data.
comments_corpus_clean <- tm_map(comments_corpus_clean, removePunctuation)
```

```{r}
#Stemming the text data to strip the suffix from words.  
comments_corpus_clean <- tm_map(comments_corpus_clean, stemDocument)
```

Final prep and data check.
```{r}
#Final step removes white space from the document. This is NOT optional.
comments_corpus_clean <- tm_map(comments_corpus_clean, stripWhitespace)
```

```{r}
#Look at an example of data.
as.character((comments_corpus_clean[[3]]))
```

####\  

**Original Tokenizer**
This version uses the "DocumentTermMatrix" function to creates a matrix in which the rows indicate individual documents (individual TA comments) and the columns indicate presence or absence of the word that is the header for the column. Be careful not to confuse it with "TermDocumentMatrix", which transposes the matrix.

"DocumentTermMaxtrix" function also can perform all of the text prep above in one command. 

```{r}
comments_corpus_1gram <- DocumentTermMatrix(comments_corpus_clean)
comments_corpus_1gram_tidy <- tidy(comments_corpus_1gram)
inspect(comments_corpus_1gram)
```

####\  

###Data Preparation
Split data into 75% training and 25% testing sets, so that after Naive Bayes spam filter algorithm is built it can be applied to unseen data. 
```{r}
.75 * 9340 #Number of rows in the dataset; product is #rows for training
.25 * 9340 #Product is #rows for testing set
```

This code assumes comments are random. **Probably want to try randomizing them.**
```{r}
comments_dtm_train <- comments_corpus_1gram[1:7005, ]
comments_dtm_test <- comments_corpus_1gram[7006:9340, ]
```

Save vectors labeling rows in the training and testing vectors
```{r}
comments_train_labels <- comments_raw[1:7005, ]$type
comments_test_labels <- comments_raw[7006:9340,]$type
```

Make sure that the proportion of each sub-category is similar in the training and testing data set.
```{r}
prop.table(table(comments_train_labels))
prop.table(table(comments_test_labels))
```

####\  

###Preparation for Naive Bayes
Remove words from the matrix that appear less than 5 times.
```{r}
comments_freq_words <- findFreqTerms(comments_dtm_train, 5)
str(comments_freq_words)
```

Limit our Document Term Matrix to only include words in the sms_freq_vector. We want all the rows, but we want to limit the column to these words in the frequency vector.
```{r}
comments_dtm_freq_train <- comments_dtm_train[ , comments_freq_words]
comments_dtm_freq_test <- comments_dtm_test[ , comments_freq_words]
```

The naive bayes classifier works with categorical features, so we need to convert the matrix to "yes" and "no" categorical variables. To do this we'll build a convert_counts function and apply it to our data.
```{r}
convert_counts2 <- function(x) {
  x <- ifelse(x > 0, "Yes", "No")
}
```

This replaces values greater than 0 with yes, and values not greater than 0 with no. Let's apply it to our data.
```{r}
comments_train <- apply(comments_dtm_freq_train, MARGIN = 2, convert_counts2)
comments_test <- apply(comments_dtm_freq_test, MARGIN = 2, convert_counts2)
```

The resulting matrixes will be character type, with cells indicating "yes" or "no" if the word represented by the column appears in the message represented by the row.

####\  

###Train Model, Predict, Evaluate
Use the e1071 package to implement Naive Bayes algorithm on the data, and predict whether a message is likely to be in group ONE, TWO, THREE, or FOUR. Evaluate the prediction with the actual data using a crosstable from the gmodels package.
```{r}
comments_classifier <- naiveBayes(comments_train, comments_train_labels)
comments_test_pred <- predict(comments_classifier, comments_test)
CrossTable(comments_test_pred, comments_test_labels, prop.chisq = FALSE, prop.t = FALSE, dnn = c('predicted', 'actual'))
```

```{r}
(42+449+1185+186)/2335
```


***
***

##Permutations of Naive Bayes (NB) Method
Version 1.0 correctly identify comment subcategories ~80% of the time. This is better than the inter-rater reliability of two human raters, but still low. This notebook documents tests of various permutations of the NB method including:

*  Compare 2-grams, 3-grams rather than single words
*  Make pair-wise comparisons versus comparing multiple groups simultaneously. 
    +  Ex.: Basic vs. Not Basic (1_Basic vs. not 1_Basic)
*  Modify the variables within the NB analysis itself. 
    +  Laplace constant (Y/N), Cutoff value for words removed from list.
*  Do/Do not stem the terms
    +  This may remove evidence of tense
*  Remove/Leave stopwords
*  Remove/Leave capitalization
*  Remove/Leave numbers
    +  This may remove evidence of technical descriptors
*  Remove/Leave punctuation
    +  This may remove evidence of questions vs statements


Data set converted to a volative corpus using "tm" library then transformed.
```{r}
#Create the volatile coprus that contains the "text" vector from data frame.
comments_corpus <- VCorpus(VectorSource(comments_raw$text))
print(comments_corpus)

#Check out the first few messages in the new corpus, which is basically a list that can be manipulated with list operations.
inspect(comments_corpus[1:3])

#Use "as.character" function to see what a message looks like.
as.character(comments_corpus[[3]])
```

The OPTIONAL data transforms using "tm_map"" and "content_transformer" functions.
```{r}
#Convert to all lower case letters.
comments_corpus_clean <- tm_map(comments_corpus, content_transformer(tolower))
```

```{r}
#Remove numbers. 
comments_corpus_clean <- tm_map(comments_corpus_clean, removeNumbers)
```

```{r}
#Stopword removal. 
comments_corpus_clean <- tm_map(comments_corpus_clean, removeWords, stopwords())
```

```{r}
#Remove punctuation as well using the "removePunctuation" function. 
#This step removes evidence of questions, so may remove data.
comments_corpus_clean <- tm_map(comments_corpus_clean, removePunctuation)
```

```{r}
#Stemming the text data to strip the suffix from words.  
comments_corpus_clean <- tm_map(comments_corpus_clean, stemDocument)
```

Final prep and data check.
```{r}
#Final step removes white space from the document. This is NOT optional.
comments_corpus_clean <- tm_map(comments_corpus_clean, stripWhitespace)
```

```{r}
#Look at an example of data.
as.character((comments_corpus_clean[[3]]))
```

####\  

**Alternative Tokenizer For N-grams**
This code block uses tm's NLP commands to generate n-grams from full corpus of TA comments. It can create 1-, 2-, and 3-grams all with the same commands simply by changing "#" in "(words(x), #)" below.

```{r}
NLP_Tokenizer <- function(x) {
      unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)
}

comments_dtm_2gram <- DocumentTermMatrix(comments_corpus_clean, control=list(tokenize = NLP_Tokenizer))
comments_dtm_2gram_tidy <- tidy(comments_dtm_2gram)
inspect(comments_dtm_2gram)
```

####\  

###Data Preparation
Split data into 75% training and 25% testing sets, so that after Naive Bayes spam filter algorithm is built it can be applied to unseen data. 

```{r}
.75 * 9129
.25 * 9129
```

This code assumes comments are random. **Probably want to try randomizing them.**
```{r}
comments_dtm_train <- comments_dtm_2gram[1:6846, ]
comments_dtm_test <- comments_dtm_2gram[6847:9129, ]
```

Save vectors labeling rows in the training and testing vectors
```{r}
comments_train_labels <- comments_raw[1:6846, ]$type
comments_test_labels <- comments_raw[6847:9129,]$type
```

Make sure that the proportion of each sub-category is similar in the training and testing data set.
```{r}
prop.table(table(comments_train_labels))
prop.table(table(comments_test_labels))
```

####\  

###Preparation for Naive Bayes
Remove words from the matrix that appear less than 5 times.
```{r}
comments_freq_words <- findFreqTerms(comments_dtm_train, 5)
str(comments_freq_words)
```

Limit our Document Term Matrix to only include words in the sms_freq_vector. We want all the rows, but we want to limit the column to these words in the frequency vector.
```{r}
comments_dtm_freq_train <- comments_dtm_train[ , comments_freq_words]
comments_dtm_freq_test <- comments_dtm_test[ , comments_freq_words]
```

The naive bayes classifier works with categorical features, so we need to convert the matrix to "yes" and "no" categorical variables. To do this we'll build a convert_counts function and apply it to our data.
```{r}
convert_counts2 <- function(x) {
  x <- ifelse(x > 0, "Yes", "No")
}
```

This replaces values greater than 0 with yes, and values not greater than 0 with no. Let's apply it to our data.
```{r}
comments_train <- apply(comments_dtm_freq_train, MARGIN = 2, convert_counts2)
comments_test <- apply(comments_dtm_freq_test, MARGIN = 2, convert_counts2)
```

The resulting matrixes will be character type, with cells indicating "yes" or "no" if the word represented by the column appears in the message represented by the row.

####\  

###Train Model, Predict, Evaluate
Use the e1071 package to implement Naive Bayes algorithm on the data, and predict whether a message is likely to be in group TWO, THREE, or FOUR. Evaluate the prediction with the actual data using a crosstable from the gmodels package.
```{r}
comments_classifier <- naiveBayes(comments_train, comments_train_labels)
comments_test_pred <- predict(comments_classifier, comments_test)
CrossTable(comments_test_pred, comments_test_labels, prop.chisq = FALSE, prop.t = FALSE, dnn = c('predicted', 'actual'))
```


```{r}
(39+219+1233+90)/2283
```

***
***


####\  

##Visualization
Create a wordcloud of the frequency of the words in the dataset using the package "wordcloud".
```{r}
wordcloud(comments_corpus_clean, max.words = 50, random.order = FALSE)
```

Compare wordclouds between 3 groups.
```{r}
ONE <- subset(comments_raw, type == "1_Basic")
TWO <- subset(comments_raw, type == "2_Writing")
THREE <- subset(comments_raw, type == "3_Technical")
FOUR <- subset(comments_raw, type == "4_Logic")

wordcloud(ONE$text, max.words = 50, scale = c(3, 0.5))
wordcloud(TWO$text, max.words = 50, scale = c(3, 0.5))
wordcloud(THREE$text, max.words = 50, scale = c(3, 0.5))
wordcloud(FOUR$text, max.words = 50, scale = c(3, 0.5))
```

Show the 5 most frequent words in the data:
```{r}
comment_sack <- TermDocumentMatrix(comments_corpus_clean)
comment_pack <- as.matrix(comment_sack)
comment_snack <- sort(rowSums(comment_pack), decreasing = TRUE)
comment_hack <- data.frame(word = names(comment_snack), freq=comment_snack)
head(comment_hack, 10)
```

And the 5 most frequent words from each class:
```{r}
wordcloud(ONE$text, max.words = 10, scale = c(3, 0.5))
wordcloud(TWO$text, max.words = 10, scale = c(3, 0.5))
wordcloud(THREE$text, max.words = 10, scale = c(3, 0.5))
wordcloud(FOUR$text, max.words = 10, scale = c(3, 0.5))
```


***
***

**Permutation 1**
Did not remove numbers, did not remove stopwords or punctuation, did not stem the terms. Reduced word count limit from 5 to 4 for matrix word limit.

```{}
comments_freq_words <- findFreqTerms(comments_dtm_train, 4)
str(comments_freq_words)
```

This step raised accuracy for calling 3_Technical comments to 91.9%, but dropped overall accuracy to 79.0%.

***
***

**Permutation 2**
Removed stopwords, numbers and punctuation, but did not use stemming. Also dropped to comparing two groups. This approach raised the accuracy to:

*  87.7% for separating 2_writing and 3_technical. 
*  83.1% for separating 2_writing and 4_logic. 
*  91.2% for separating 3_technical and 4_logic. 


```{r}
library(tidyverse)
library(tidytext)
library(dplyr)
library(tidyr)
library(ggplot2)
library(scales)
library(tm)
library(SnowballC)
library(wordcloud)
library(e1071)
library(gmodels)
library(readr)
```

```{r}
#Read in CSV file named "coded_full_comments_dataset_Spring18anon.csv".
base_data <- read_csv(file='data/coded_full_comments_dataset_Spring18anon.csv')

#Isolate rows to compare. 
comments_subset <- filter(base_data,code.subject=="3. Technical and Scientific"|code.subject=="4. Logic and Thinking") #|code.subject=="2. Writing Quality"

#Reduce larger dataframe to 2 required columns of data, and put columns in order needed.
comments_raw <- comments_subset %>% select(23,22)

#Rename the columns.
names(comments_raw)[1] <- "type"
names(comments_raw)[2] <- "text"

#Simplify coding terms
comments_raw[,1] <- ifelse(comments_raw[,1] == "3. Technical and Scientific", "3_Technical", ifelse(comments_raw[,1] == "4. Logic and Thinking","4_Logic", 99))# ifelse(comments_raw[,1] == ,99)))"2. Writing Quality", "2_Writing"

#Change "type" element from character to a factor for analysis.
comments_raw$type <- factor(comments_raw$type)
str(comments_raw$type)
table(comments_raw$type)
```

```{r}
##Text Transformation using "tm" package in R to create a volitile coprus that contains the "text" vector from our data frame.
comments_corpus <- VCorpus(VectorSource(comments_raw$text))
print(comments_corpus)

#Check out the first few messages in the new corpus, which is basically a list that can be manipulated with list operations.
inspect(comments_corpus[1:3])

#Use "as.character" function to see what a message looks like.
as.character(comments_corpus[[3]])
```

In order to standardize the messages, the data set must be tranformed to all lower case letters. The words "Free", "free", and "FREE" should all be treated as the same word. Use the "tm_map"" funtion in R, and use the "content_transformer" function to transform the text.
```{r}
comments_corpus_clean <- tm_map(comments_corpus, content_transformer(tolower))
```

Look at third message again to see if our data was transformed.
```{r}
as.character(comments_corpus[[3]])

as.character((comments_corpus_clean[[3]]))
```

This removes numbers. **May not want to do.**
```{r}
comments_corpus_clean <- tm_map(comments_corpus_clean, removeNumbers)
```

Stopword removal takes out words that appear often but don't contribute to our objective. These words include "to", "and", "but" and "or". **One of variables to test out.**
```{r}
comments_corpus_clean <- tm_map(comments_corpus_clean, removeWords, stopwords())
```

Remove punctuation as well using the "removePunctuation" function. **Try not doing this. Removes evidence of questions.**
```{r}
comments_corpus_clean <- tm_map(comments_corpus_clean, removePunctuation)
as.character((comments_corpus_clean[[3]]))
```

Perform "stemming" to the text data to strip the suffix from words. **Try NOT doing this.** 
```{}
comments_corpus_clean <- tm_map(comments_corpus_clean, stemDocument)
```

And now the final step in text mining is to remove white space from the document. 
```{r}
comments_corpus_clean <- tm_map(comments_corpus_clean, stripWhitespace)
```

```{r}
as.character(comments_corpus_clean[[3]])
```

Perform tokenization using the "DocumentTermMatrix" function. This creates a matrix in which the rows indicate documents (SMS messages in this case) and the columns indicate words. It should be noted that the "DocumentTermMaxtrix" function has the power to do all of the text mining above in one command.
```{r}
sms_dtm2 <- DocumentTermMatrix(comments_corpus_clean)
```

####\  

##Data Preparation
Split data into training and testing sets, so that after Naive Bayes spam filter algorithm is built it can be applied to unseen data. Divide our data set into 75% training and 25% testing.

```{r}
.75 * 6551

.25 * 9129
```

This code assumes comments are random. **Probably want to try randomizing them.**
```{r}
comments_dtm_train <- sms_dtm2[1:4913, ]
comments_dtm_test <- sms_dtm2[4914:6551, ]
```

Save vectors labeling rows in the training and testing vectors
```{r}
comments_train_labels <- comments_raw[1:4913, ]$type
comments_test_labels <- comments_raw[4914:6551,]$type
```

Make sure that the proportion of each sub-category is similar in the training and testing data set.
```{r}
prop.table(table(comments_train_labels))
prop.table(table(comments_test_labels))
```

####\  

##Visualization
Create a wordcloud of the frequency of the words in the dataset using the package "wordcloud".
```{}
wordcloud(comments_corpus_clean, max.words = 50, random.order = FALSE)
```

Compare wordclouds between 3 groups.
```{}
#TWO <- subset(comments_raw, type == "2_Writing")
THREE <- subset(comments_raw, type == "3_Technical")
FOUR <- subset(comments_raw, type == "4_Logic")

#wordcloud(TWO$text, max.words = 50, scale = c(3, 0.5))
wordcloud(THREE$text, max.words = 50, scale = c(3, 0.5))
wordcloud(FOUR$text, max.words = 50, scale = c(3, 0.5))
```

####\  

##Preparation for Naive Bayes
Remove words from the matrix that appear less than 5 times.
```{r}
comments_freq_words <- findFreqTerms(comments_dtm_train, 5)
str(comments_freq_words)
```

Limit our Document Term Matrix to only include words in the sms_freq_vector. We want all the rows, but we want to limit the column to these words in the frequency vector.
```{r}
comments_dtm_freq_train <- comments_dtm_train[ , comments_freq_words]
comments_dtm_freq_test <- comments_dtm_test[ , comments_freq_words]
```

The naive bayes classifier works with categorical features, so we need to convert the matrix to "yes" and "no" categorical variables. To do this we'll build a convert_counts function and apply it to our data.
```{r}
convert_counts2 <- function(x) {
  x <- ifelse(x > 0, "Yes", "No")
}
```

This replaces values greater than 0 with yes, and values not greater than 0 with no. Let's apply it to our data.
```{r}
comments_train <- apply(comments_dtm_freq_train, MARGIN = 2, convert_counts2)
comments_test <- apply(comments_dtm_freq_test, MARGIN = 2, convert_counts2)
```

The resulting matrixes will be character type, with cells indicating "yes" or "no" if the word represented by the column appears in the message represented by the row.

####\  

##Train Model, Predict, Evaluate
Use the e1071 package to implement Naive Bayes algorithm on the data, and predict whether a message is likely to be in group TWO, THREE, or FOUR. Evaluate the prediction with the actual data using a crosstable from the gmodels package.
```{r}
comments_classifier <- naiveBayes(comments_train, comments_train_labels)
comments_test_pred <- predict(comments_classifier, comments_test)
CrossTable(comments_test_pred, comments_test_labels, prop.chisq = FALSE, prop.t = FALSE, dnn = c('predicted', 'actual'))
```

Show the 10 most frequent words in the data:
```{r}
sack2 <- TermDocumentMatrix(comments_corpus_clean)
pack2 <- as.matrix(sack2)
snack2 <- sort(rowSums(pack2), decreasing = TRUE)
hack2 <- data.frame(word = names(snack2), freq=snack2)
head(hack2, 10)
```

And the 10 most frequent words from each class:
```{}
wordcloud(TWO$text, max.words = 10, scale = c(3, 0.5))
wordcloud(THREE$text, max.words = 10, scale = c(3, 0.5))
#wordcloud(FOUR$text, max.words = 10, scale = c(3, 0.5))
```

