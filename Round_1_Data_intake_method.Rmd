---
title: "Round 1 Data Intake Method"
author: "Dan Johnson"
date: "11/24/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##General Approach and Initial Dataset
The following excerpt from *Text Mining with R: A Tidy Approach* (Julia Silge and David Robinson) summarizes how extended text (in this case, TA comments) can be organized as a tidy matrix.*All emphases are mine.*

>We thus define the tidy text format as being a table with one-token-per-row. A token is a **meaningful unit of text**, such as a word, that we are interested in using for analysis, and tokenization is the process of splitting text into tokens. 

>This one-token-per-row structure is in contrast to the ways text is often stored in current analyses, perhaps as strings or in a document-term matrix. For tidy text mining, the token that is stored in each row is most often a single word, **but can also be an n-gram, sentence, or paragraph** . In the tidytext package, we provide functionality to tokenize by commonly used units of text like these and convert to a one-term-per-row format.

The initial working dataset is an anonymized Excel table with one column of "TA comments" extracted from student lab reports. Each TA comment is stored as a separate row of the table, and is a mixed alphanumeric string of one or more words, numbers, and punctuation. Other columns record unique report, student, and TA IDs; grade assigned to the report; other standardized information about the original report from which the comment was extracted; and the hand-coded subject and structure of each comment. Using tidy format (vs. other common text data formats) better maintains the relationships between individual comments and metadata, which simplifies subsequent analysis. 

##Preparing and Pre-Checking the Initial Dataset
BEFORE starting, review data table in Excel:

1.  Export to a .CSV file with name formatted as **"coded_full_comments_dataset_SemYear.csv"**
2.  Check that coding vocabulary terms used in coding columns matches criteria in "codebook_for_comments.md"
3.  Data have been de-identified and recoded using pre-assigned anonymous IDs.
4.  Check that data table headers are in snake_case, and match list below. The same list is in the file "code_column_names.txt"

Column Number|Column Name
------------|----------------------
1|unique.record
2|report.id
3|sort
4|report.title (empty)
5|student (Std\_nnn)
6|course
7|ta (TA\_nnn)
8|lab
9|tag
10|type.TA
11|grade.TA
12|grading.time
13|Rank
14|hypothesis.ok
15|data.ok
16|citation.ok
17|interpretation.ok
18|organization.ok
19|techflaws.ok
20|writing.ok
21|comments.incorporated
22|ta.comment
23|code.subject
24|code.structure
25|code.locus
26|code.scope
27|code.tone
28|code.notes

##Import Data
Call in tidyverse library
```{r Libraries}
library(tidyverse)
library(tidytext)
library(dplyr)
library(tidyr)
```

Read in the raw dataset from .CSV file to a data frame with name "base_data".

```{r}
base_data <- read_csv(file='data/coded_full_comments_dataset_Spring18anon.csv')
head(base_data)
```

##Run Post-Import Data Checks
These checks ensure data have been properly coded and columns all contain valid entries. 

"Course" column should only have entries that correspond to 113, 114, 214, NA. 
```{r}
unique(base_data$course)
```

"TAs" column should match the anonymous IDs for TAs assigned to the relevant courses in the semester being analyzed. Check that no additional or extra anonymous IDs are present, which would suggest improper coding or de-identification.

```{r}
unique(base_data$ta)
```

"Lab" should only have entries that match the limited keywords in codebook.
```{r}
unique(base_data$lab)
```

Look at the NUMBER of unique anonymous student IDs. Should be within 5% of (but not over) the combined enrollment of the 3 target courses for the semester.
```{r}
unique(base_data$student)
```

Check that the "code.subject" list extracted from the dataset matches allowed terms in codebook.
```{r}
unique(base_data$code.subject)
```

Check that the "code.structure" list extracted from the dataset matches allowed terms in codebook.
```{r}
unique(base_data$code.structure)
```


## Extract and Tokenize Text of Comments
This block tokenizes the whole set of comments, and keeps other meta-data intact and attached to the tokens. In the new version of base_data, a new column at FAR RIGHT called "word" contains all words in the "ta.comment" column.

Post-check: Column 28 now has title "word", "ta.comment" has been removed. Other data should have duplicated along with the words for each unnested comment. 

```{r}
base_data_tokenized <- base_data %>% 
  unnest_tokens(word,ta.comment)
```



##Summarize Word Frequencies in FULL Set of Comments
Take all comments from base_data, unnest tokens, filter stopwords, count and sort remaining terms, then writing back to a new datatable called 'sortedwords.comments.all'.  

Calculate TOTAL words and store. Then calculate fractional frequencies for each word and store using mutate.

```{r}
library(dplyr)
library(tidytext)

sortedwords.comments.all <- base_data_tokenized %>%
  filter(!word %in% stop_words$word,
         !word %in% str_remove_all(stop_words$word, "'"),
         str_detect(word, "[a-z]")) %>%
  count(word, word, sort = TRUE) %>%
  ungroup()

#Calculate the number of words in total in the subset
total_words.all <- sortedwords.comments.all %>% 
  summarize(total = sum(n))

#Mutate table to calculate then append fractional word frequency in the text.
sortedwords.comments.all.fraction <- sortedwords.comments.all %>% as_tibble() %>% mutate(
  all.fraction = (n*100)/total_words.all$total
  )
```


##Summarize Word Frequencies in Subsets of Comments Based on 'Subject'
Take subsets of comments from base_data_tokenized using code.subject as separating characteristic.
```{r}
#Pull SUBSETs of tokenized data based on code.subject: 
base_data_tokenized.basic <- subset(base_data_tokenized, code.subject == "1. Basic Criteria", select = (1:28))
base_data_tokenized.writing <- subset(base_data_tokenized, code.subject == "2. Writing Quality", select = (1:28))
base_data_tokenized.technical <- subset(base_data_tokenized, code.subject == "3. Technical and Scientific", select = (1:28))
base_data_tokenized.logic <- subset(base_data_tokenized, code.subject == "4. Logic and Thinking", select = (1:28))
base_data_tokenized.praise <- subset(base_data_tokenized, code.subject == "5. Praise or Concern", select = (1:28))
base_data_tokenized.misconduct <- subset(base_data_tokenized, code.subject == "6. Misconduct", select = (1:28))
base_data_tokenized.narrative <- subset(base_data_tokenized, code.subject == "12. Narrative Comments", select = (1:28))
base_data_tokenized.sciname <- subset(base_data_tokenized, code.subject == "Scientific Name", select = (1:28))
```


For each subset, filter stopwords, count words and total, then calculate frequencies.

```{r}
sortedwords.basic <- base_data_tokenized.basic %>%
  filter(!word %in% stop_words$word,
         !word %in% str_remove_all(stop_words$word, "'"),
         str_detect(word, "[a-z]")) %>%
  count(word, word, sort = TRUE) %>%
  ungroup()

#Calculate the number of words in total in the subset
total_words.basic <- sortedwords.basic %>% 
  summarize(total = sum(n))

#Mutate table to calculate then append fractional word frequency in the text.
sortedwords.basic.fraction <- sortedwords.basic %>% as_tibble() %>% mutate(
  basic.fraction = (n*100)/total_words.basic$total
)
```

```{r}
sortedwords.writing <- base_data_tokenized.writing %>%
  filter(!word %in% stop_words$word,
         !word %in% str_remove_all(stop_words$word, "'"),
         str_detect(word, "[a-z]")) %>%
  count(word, word, sort = TRUE) %>%
  ungroup()

#Calculate the number of words in total in the subset
total_words.writing <- sortedwords.writing %>% 
  summarize(total = sum(n))

#Mutate table to calculate then append fractional word frequency in the text.
sortedwords.writing.fraction <- sortedwords.writing %>% as_tibble() %>% mutate(
  writing.fraction = (n*100)/total_words.writing$total
)
```

```{r}
sortedwords.technical <- base_data_tokenized.technical %>%
  filter(!word %in% stop_words$word,
         !word %in% str_remove_all(stop_words$word, "'"),
         str_detect(word, "[a-z]")) %>%
  count(word, word, sort = TRUE) %>%
  ungroup()

#Calculate the number of words in total in the subset
total_words.technical <- sortedwords.technical %>% 
  summarize(total = sum(n))

#Mutate table to calculate then append fractional word frequency in the text.
sortedwords.technical.fraction <- sortedwords.technical %>% as_tibble() %>% mutate(
  technical.fraction = (n*100)/total_words.technical$total
)
```

```{r}
sortedwords.logic <- base_data_tokenized.logic %>%
  filter(!word %in% stop_words$word,
         !word %in% str_remove_all(stop_words$word, "'"),
         str_detect(word, "[a-z]")) %>%
  count(word, word, sort = TRUE) %>%
  ungroup()

#Calculate the number of words in total in the subset
total_words.logic <- sortedwords.logic %>% 
  summarize(total = sum(n))

#Mutate table to calculate then append fractional word frequency in the text.
sortedwords.logic.fraction <- sortedwords.logic %>% as_tibble() %>% mutate(
  logic.fraction = (n*100)/total_words.logic$total
)
```

Join the individual data tables for each subset in a larger file using column named 'word'. Columns get confusing names so they are renamed.

**WARNING**: Words that are MISSING from **any subset** cause the full set to be pruned. I am losing the unique words this way. **Need to figure out how to join without pruning.**


```{r}
total <- merge(sortedwords.comments.all.fraction,sortedwords.writing.fraction,by="word") 
total2 <- merge(total,sortedwords.technical.fraction,by="word")
total3 <- merge(total2,sortedwords.logic.fraction,by="word")
#rename lines that were possibly confusing
names(total3)[2] <- "all.count"
names(total3)[4] <- "writing.count"
names(total3)[6] <- "technical.count"
names(total3)[8] <- "logic.count"

#Mutate table to calculate then append fractional word frequency in the text.
total4 <- total3 %>% as_tibble() %>% mutate(
  writing.wt = (writing.count *100)/all.count
)

total5 <- total4 %>% as_tibble() %>% mutate(
  technical.wt = (technical.count *100)/all.count
)

total6 <- total5 %>% as_tibble() %>% mutate(
  logic.wt = (logic.count *100)/all.count
)

```


To Do Next:

Now I need to cross-graph the weights for each word relative to others. 



***
***


###Alternative Options
For other types of analyses I might want to extract text using other methods shown below:

**Option 1.**  Create a single-column data frame of just the comments. Then check that the data was written to the file as characters not factors.
```{r}
base_data_comments <- as.data.frame(base_data[,22], drop=FALSE, stringsAsFactors=FALSE) #assumes Col. 22 is the ta.commment column
class(base_data_comments[23,1]) #Should report as "character"
```

**Option 2.** Create a NEW data frame for analysis. Contains **ta.comment** (Col. 22), **code.subject** (Col. 23) and **code.structure** (Col. 24). Again, check that written in data are characters, not factors. 
```{r}
base_data_3columns <- as.data.frame(base_data[,c(22:24)], drop=FALSE, stringsAsFactors=FALSE)
(base_data_3columns[33:35,1:3]) 
#First column should be row numbers, 2nd to 4th columns of class "chr"
```

**Option 3.** Import comments as subset blocks
This is an ugly set of steps to create dataframes for analysis. The output subsets of data from the full base_data table can be analyzed as separate groups. Only benefit I see is they are COMPLETELY INDEPENDENT of one another.
```{r}
#The columns to be included in each subset table
colnames.base_data <-  c("unique.record","ta.comment","code.subject","code.structure")
# These first lines create a subset table for ALL lines in the table.
base_data_3columns.all <- subset(base_data, select = colnames.base_data)
base_data_comments.all <- as.data.frame(base_data_3columns.all[,2], drop=FALSE, stringsAsFactors=FALSE) #assumes Col. 2 is the ta.commment column

# The first line of each pair from here down creates subset tables based on what is in the "code.subject" data column. The second line of each pair extracts just the TA comments so they can be tokenized.
base_data_3columns.basic <- subset(base_data, code.subject == "1. Basic Criteria", select = colnames.base_data)
base_data_comments.basic <- as.data.frame(base_data_3columns.basic[,2], drop=FALSE, stringsAsFactors=FALSE) #assumes Col. 2 is the ta.commment column

base_data_3columns.writing <- subset(base_data, code.subject == "2. Writing Quality", select = colnames.base_data)
base_data_comments.writing <- as.data.frame(base_data_3columns.writing[,2], drop=FALSE, stringsAsFactors=FALSE) #assumes Col. 2 is the ta.commment column

base_data_3columns.technical <- subset(base_data, code.subject == "3. Technical and Scientific", select = colnames.base_data)
base_data_comments.technical <- as.data.frame(base_data_3columns.technical[,2], drop=FALSE, stringsAsFactors=FALSE) #assumes Col. 2 is the ta.commment column

base_data_3columns.logic <- subset(base_data, code.subject == "4. Logic and Thinking", select = colnames.base_data)
base_data_comments.logic <- as.data.frame(base_data_3columns.logic[,2], drop=FALSE, stringsAsFactors=FALSE) #assumes Col. 2 is the ta.commment column

base_data_3columns.praise <- subset(base_data, code.subject == "5. Praise or Concern", select = colnames.base_data)
base_data_comments.praise <- as.data.frame(base_data_3columns.praise[,2], drop=FALSE, stringsAsFactors=FALSE) #assumes Col. 2 is the ta.commment column

base_data_3columns.misconduct <- subset(base_data, code.subject == "6. Misconduct", select = colnames.base_data)
base_data_comments.misconduct <- as.data.frame(base_data_3columns.misconduct[,2], drop=FALSE, stringsAsFactors=FALSE) #assumes Col. 2 is the ta.commment column

base_data_3columns.narrative <- subset(base_data, code.subject == "12. Narrative Comments", select = colnames.base_data)
base_data_comments.narrative <- as.data.frame(base_data_3columns.narrative[,2], drop=FALSE, stringsAsFactors=FALSE) #assumes Col. 2 is the ta.commment column

base_data_3columns.sciname <- subset(base_data, code.subject == "Scientific Name", select =  colnames.base_data)
base_data_comments.sciname <- as.data.frame(base_data_3columns.sciname[,2], drop=FALSE, stringsAsFactors=FALSE) #assumes Col. 2 is the ta.commment column
```

This second series of commands creates tokenized words lists for all comments together, and token subsets for each comments subset.
```{r}
base_data_comments.all.tokens <- base_data_comments %>% 
  unnest_tokens(word,ta.comment)
base_data_comments.basic.tokens <- base_data_comments.basic %>% 
  unnest_tokens(word,ta.comment)
base_data_comments.writing.tokens <- base_data_comments.writing %>% 
  unnest_tokens(word,ta.comment)
base_data_comments.technical.tokens <- base_data_comments.technical %>% 
  unnest_tokens(word,ta.comment)
base_data_comments.logic.tokens <- base_data_comments.logic %>% 
  unnest_tokens(word,ta.comment)
base_data_comments.praise.tokens <- base_data_comments.praise %>% 
  unnest_tokens(word,ta.comment)
base_data_comments.misconduct.tokens <- base_data_comments.misconduct %>% 
  unnest_tokens(word,ta.comment)
base_data_comments.narrative.tokens <- base_data_comments.narrative %>% 
  unnest_tokens(word,ta.comment)
base_data_comments.sciname.tokens <- base_data_comments.sciname %>% 
  unnest_tokens(word,ta.comment)

#Display the last set of tokens to show process finished.
base_data_comments.sciname.tokens
```


***

# Tokenizing Text of Comments
Create a single-column data frame of just the comments for ALL lines. Then check that the data was written to the file as characters not factors.



####If you must convert data to a Tibble:
To make tokenizing work you must convert the desired set of extracted data into a tibble. You HAVE to use a tibble to tokenize text lines. Trying to do it straight from a values table will throw an error.

```
base_data_comments_50_tibble <- tibble(line = seq_along(data), text = base_data_comments_50(c[,2])) #tibble-izer

#base_data_comments_50_tibble %>% 
#  unnest_tokens(word,ta.comment)
```
unique.record, code.subject, code.structure
base_data


***
***

##FWIW
This command uses a tibble with 3 data columns, and successfully unnests the tokens. It is messy because the other columns are duplicated, BUT at least it is possible to wrangle a multi-column tibble for tokenization.
```{r}
Abase_data_3columns.sciname.tokens <- base_data_3columns.sciname %>% 
  unnest_tokens(word,ta.comment)
```



***
#Approach ONE: Work backwards from existing sorting to get criteria.
I have a pre-defined set of traits. Can I build from that?

Do it using naive Bayes. 

##Step 1.1: Regenerate the groups in two-dimensional array


##Step 1.2: Yada


##Step 1.3: Yada


##Step 1.4: Yada


***
#Approach TWO: Assume no a priori groups, and look for natural clusters. 



##Step 2.1: Yada


##Step 2.2: Yada


##Step 2.3: Yada


##Step 2.4: Yada

