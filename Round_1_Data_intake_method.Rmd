---
title: "Round 1 Data Intake Method"
author: "Dan Johnson"
date: "11/24/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##Prep
Call in tidyverse library
```{r}
library(tidyverse)
library(tidytext)
library(dplyr)
library(tidyr)
```

From *Text Mining with R: A Tidy Approach* (Julia Silge and David Robinson)

>We thus define the tidy text format as being a table with one-token-per-row. A token is a meaningful unit of text, such as a word, that we are interested in using for analysis, and tokenization is the process of splitting text into tokens. 

>This one-token-per-row structure is in contrast to the ways text is often stored in current analyses, perhaps as strings or in a document-term matrix. For tidy text mining, the token that is stored in each row is most often a single word, but can also be an n-gram, sentence, or paragraph. In the tidytext package, we provide functionality to tokenize by commonly used units of text like these and convert to a one-term-per-row format.

My working dataset for analysis is organized with one "instructor comment" as the one token per row in the table. Each instructor comment is an alphanumeric string of multiple words. 


##Pre-Check Dataset
BEFORE starting, review data table in Excel:

1.  Export to a .CSV file named as **"coded_full_comments_dataset_SemYear.csv"**
2.  Check that data table headers are in snake_case.
3.  Check that coding vocabulary terms used in coding columns matches criteria in "codebook_for_comments.md"
4.  Check that data table header names match list below, which also should be in "code_column_names.txt"

Column Number|Column Name
------------|----------------------
1|unique.record
2|report.id
3|sort
4|report.title
5|student
6|course
7|ta
8|lab
9|tag
10|type.TA
11|grade.TA
12|grading.time
13|Rank
14|hypothesis.ok
15|data.ok
16|citation.ok
17|interpretation.ok
18|organization.ok
19|techflaws.ok
20|writing.ok
21|comments.incorporated
22|ta.comment
23|code.subject
24|code.structure
25|code.locus
26|code.scope
27|code.tone
28|code.notes



##Import Data
Read in the raw dataset from .CSV file to a data frame with name "base_data".

```{r}
base_data <- read_csv(file='data/coded_full_comments_dataset_Spring18.csv')
head(base_data)
```

***

##De-Identify TAs and Students in base_data
**In progress.**

In this step I need to mutate "base_data" to create a new file where TA names and student email addresses are replaced with pre-assigned de-identifiers.

First I created a set of uniform anonymous identifiers, put them in a CSV, and can import them to serve as the lookup tables for TAs, student emails. This code reads in the raw dataset containing TA names, student email addresses, and corresponding anonymous IDs, from xxx.CSV file and creates TWO data frames.

```{r}
TA_identifiers <- read_csv(file='data/ID_lookup.csv')[1:15,2:1]
#assumes Cols. 1-2 are the TA_IDs and TA names, and are not the full range of rows, so we only want rows 1-15.
class(TA_identifiers) #Should report as "tbl_df"
```

```{r}
Student_identifiers <- read_csv(file='data/ID_lookup.csv')[,5:4]
#assumes Cols. 4-5 are the Student_IDs and Student email addresses.
class(Student_identifiers) #Should report as "tbl_df"
```

Okay so far. Problem is I cannot get next step of lookup process to work. I tried several methods, and none of the elegant ones seem to work. I used this Overflow as my starting point:

https://stackoverflow.com/questions/35636315/replace-values-in-a-dataframe-based-on-lookup-table

Suggestions:
1. Use JOIN to take two separate tables of data and combine them. Use GATHER to reorganize existing data. Unclear how to implement. 

2. From a usergroup:
I am having some trouble replacing values in a dataframe. I would like to replace values based on a separate table. Below is an example of what I am trying to do.

I have a table where every row is a customer and every column is an animal they purchased. Lets call this dataframe Original_table.

> Original_table
#       P1     P2     P3
# 1    cat lizard parrot
# 2 lizard parrot    cat
# 3 parrot    cat lizard

I also have a table that I will reference called lookUp.

> lookUp
#      pet   class
# 1    cat  mammal
# 2 lizard reptile
# 3 parrot    bird

What I want to do is create a new table called new with a function replaces all values in table with the class column in lookUp.

What readers recommended:

*Option A:*
```
library(dplyr)
library(tidyr)
Original_table %>%
   gather(key = "pet") %>%
   left_join(lookUp, by = "pet") %>%
   spread(key = pet, value = class)
```

Above mechanism failed. None of other recommended methods seemed to work. 


This alternative IS UGLY BUT GENERATES WHAT I NEED in a single column tibble. 
1. Create one column of TAs, then using **lapply,** loop over column and match values to the look up table. 
2. Store in list, then convert list to tibble.
3. Need to merge back in; that command is missing.

```{r}
deidentified_data2<-base_data$ta #Extract just TA name from base_data

deidentified_data2 <- lapply(base_data$ta, function(x) TA_identifiers$TA_ID[match(x, TA_identifiers$ta)]) #Loop over column and replace.

deidentified_data3 <- tibble(line = seq_along(data), text = deidentified_data2) #tibble-izer; converts the list into a tibble.
```


##Run Post-Import Data Checks
Check that data have been properly coded. 

"Course" column should only have entries that correspond to 113, 114, 214, NA. 
```{r}
unique(base_data$course)
```

Check that the names of the TAs for the dataset match the TAs assigned to the relevant courses in the semester being analyzed. Also check that no TA names appear twice, which would mean their name has been entered two or more ways.

```{r}
unique(base_data$ta)
```

"Lab" should only have entries that match the limited keywords in codebook.
```{r}
unique(base_data$lab)
```

Look at the NUMBER of unique student email addresses. Should be within 5% of (but not over) the combined enrollment of the target courses for the semester.
```{r}
unique(base_data$student)
```

Check that the "code.subject" list extracted from the dataset matches allowed terms in codebook.
```{r}
unique(base_data$code.subject)
```

Check that the "code.structure" list extracted from the dataset matches allowed terms in codebook.
```{r}
unique(base_data$code.structure)
```



## Extracting Text of Comments
The first option to break down the text of comments is to tokenize the whole set at once. This leaves the other meta-data intact and attached to the tokens. This command block creates a new version of base_data that has the new column at FAR RIGHT called "word" that contains all words in the "ta.comment" column.

After running, check that Column 28 has title "word", that "ta.comment" has been removed, and other lines have duplicated along with the words for each unnested comment. 

```{r}
base_data_tokenized <- base_data %>% 
  unnest_tokens(word,ta.comment)
```


##Summarizing Word Frequencies in ONE Set of Comments
Create 'sortedwords.comments.all' by taking all comments from base_data, unnesting tokens, filtering stopwords, counting and sorting, then writing back to a new datatable. Then calculate TOTAL words and store. Calculate fractional frequencies and store using mutate.

```{r}
library(dplyr)
library(tidytext)

sortedwords.comments.all <- base_data_tokenized %>%
  filter(!word %in% stop_words$word,
         !word %in% str_remove_all(stop_words$word, "'"),
         str_detect(word, "[a-z]")) %>%
  count(word, word, sort = TRUE) %>%
  ungroup()

#Calculate the number of words in total in the subset
total_words.all <- sortedwords.comments.all %>% 
  summarize(total = sum(n))

#Mutate table to calculate then append fractional word frequency in the text.
sortedwords.comments.all.fraction <- sortedwords.comments.all %>% as_tibble() %>% mutate(
  all.fraction = (n*100)/total_words.all$total
  )
```


This block extracts the full sets of data for unnested tokens of each of the coded subject groups. 

```{r}
#Pull SUBSETs of tokenized data based on code.subject: 
base_data_tokenized.basic <- subset(base_data_tokenized, code.subject == "1. Basic Criteria", select = (1:28))
base_data_tokenized.writing <- subset(base_data_tokenized, code.subject == "2. Writing Quality", select = (1:28))
base_data_tokenized.technical <- subset(base_data_tokenized, code.subject == "3. Technical and Scientific", select = (1:28))
base_data_tokenized.logic <- subset(base_data_tokenized, code.subject == "4. Logic and Thinking", select = (1:28))
base_data_tokenized.praise <- subset(base_data_tokenized, code.subject == "5. Praise or Concern", select = (1:28))
base_data_tokenized.misconduct <- subset(base_data_tokenized, code.subject == "6. Misconduct", select = (1:28))
base_data_tokenized.narrative <- subset(base_data_tokenized, code.subject == "12. Narrative Comments", select = (1:28))
base_data_tokenized.sciname <- subset(base_data_tokenized, code.subject == "Scientific Name", select = (1:28))
```


Need to repeat or reiterate to remove the stopwords, count words in each sub-group, and calculate frequencies.

```{r}
sortedwords.basic <- base_data_tokenized.basic %>%
  filter(!word %in% stop_words$word,
         !word %in% str_remove_all(stop_words$word, "'"),
         str_detect(word, "[a-z]")) %>%
  count(word, word, sort = TRUE) %>%
  ungroup()

#Calculate the number of words in total in the subset
total_words.basic <- sortedwords.basic %>% 
  summarize(total = sum(n))

#Mutate table to calculate then append fractional word frequency in the text.
sortedwords.basic.fraction <- sortedwords.basic %>% as_tibble() %>% mutate(
  basic.fraction = (n*100)/total_words.basic$total
)
```

```{r}
sortedwords.writing <- base_data_tokenized.writing %>%
  filter(!word %in% stop_words$word,
         !word %in% str_remove_all(stop_words$word, "'"),
         str_detect(word, "[a-z]")) %>%
  count(word, word, sort = TRUE) %>%
  ungroup()

#Calculate the number of words in total in the subset
total_words.writing <- sortedwords.writing %>% 
  summarize(total = sum(n))

#Mutate table to calculate then append fractional word frequency in the text.
sortedwords.writing.fraction <- sortedwords.writing %>% as_tibble() %>% mutate(
  writing.fraction = (n*100)/total_words.writing$total
)
```

```{r}
sortedwords.technical <- base_data_tokenized.technical %>%
  filter(!word %in% stop_words$word,
         !word %in% str_remove_all(stop_words$word, "'"),
         str_detect(word, "[a-z]")) %>%
  count(word, word, sort = TRUE) %>%
  ungroup()

#Calculate the number of words in total in the subset
total_words.technical <- sortedwords.technical %>% 
  summarize(total = sum(n))

#Mutate table to calculate then append fractional word frequency in the text.
sortedwords.technical.fraction <- sortedwords.technical %>% as_tibble() %>% mutate(
  technical.fraction = (n*100)/total_words.technical$total
)
```

```{r}
sortedwords.logic <- base_data_tokenized.logic %>%
  filter(!word %in% stop_words$word,
         !word %in% str_remove_all(stop_words$word, "'"),
         str_detect(word, "[a-z]")) %>%
  count(word, word, sort = TRUE) %>%
  ungroup()

#Calculate the number of words in total in the subset
total_words.logic <- sortedwords.logic %>% 
  summarize(total = sum(n))

#Mutate table to calculate then append fractional word frequency in the text.
sortedwords.logic.fraction <- sortedwords.logic %>% as_tibble() %>% mutate(
  logic.fraction = (n*100)/total_words.logic$total
)
```

Next I need to join the individual data tables 
  tables are sortedwords.NNN.fraction 
  column names are NNN.fraction
  Join BY column named "word"

```{r}
total <- merge(sortedwords.comments.all.fraction,sortedwords.writing.fraction,by="word") 
total2 <- merge(total,sortedwords.technical.fraction,by="word")
total3 <- merge(total2,sortedwords.logic.fraction,by="word")
#rename lines that were possibly confusing
names(total3)[2] <- "all.count"
names(total3)[4] <- "writing.count"
names(total3)[6] <- "technical.count"
names(total3)[8] <- "logic.count"

#Mutate table to calculate then append fractional word frequency in the text.
total4 <- total3 %>% as_tibble() %>% mutate(
  writing.wt = (writing.count *100)/all.count
)

total5 <- total4 %>% as_tibble() %>% mutate(
  technical.wt = (technical.count *100)/all.count
)

total6 <- total5 %>% as_tibble() %>% mutate(
  logic.wt = (logic.count *100)/all.count
)

```


To Do Next:
  WARNING: Words that are MISSING from any subset cause the full set to be pruned. I am losing the unique words this way.

Now I need to cross-graph the weights for each word relative to others. 



***
***


###Alternative Options
For other types of analyses I might want to extract text using other methods shown below:

**Option 1.**  Create a single-column data frame of just the comments. Then check that the data was written to the file as characters not factors.
```{r}
base_data_comments <- as.data.frame(base_data[,22], drop=FALSE, stringsAsFactors=FALSE) #assumes Col. 22 is the ta.commment column
class(base_data_comments[23,1]) #Should report as "character"
```

**Option 2.** Create a NEW data frame for analysis. Contains **ta.comment** (Col. 22), **code.subject** (Col. 23) and **code.structure** (Col. 24). Again, check that written in data are characters, not factors. 
```{r}
base_data_3columns <- as.data.frame(base_data[,c(22:24)], drop=FALSE, stringsAsFactors=FALSE)
(base_data_3columns[33:35,1:3]) 
#First column should be row numbers, 2nd to 4th columns of class "chr"
```

**Option 3.** Import comments as subset blocks
This is an ugly set of steps to create dataframes for analysis. The output subsets of data from the full base_data table can be analyzed as separate groups. Only benefit I see is they are COMPLETELY INDEPENDENT of one another.
```{r}
#The columns to be included in each subset table
colnames.base_data <-  c("unique.record","ta.comment","code.subject","code.structure")
# These first lines create a subset table for ALL lines in the table.
base_data_3columns.all <- subset(base_data, select = colnames.base_data)
base_data_comments.all <- as.data.frame(base_data_3columns.all[,2], drop=FALSE, stringsAsFactors=FALSE) #assumes Col. 2 is the ta.commment column

# The first line of each pair from here down creates subset tables based on what is in the "code.subject" data column. The second line of each pair extracts just the TA comments so they can be tokenized.
base_data_3columns.basic <- subset(base_data, code.subject == "1. Basic Criteria", select = colnames.base_data)
base_data_comments.basic <- as.data.frame(base_data_3columns.basic[,2], drop=FALSE, stringsAsFactors=FALSE) #assumes Col. 2 is the ta.commment column

base_data_3columns.writing <- subset(base_data, code.subject == "2. Writing Quality", select = colnames.base_data)
base_data_comments.writing <- as.data.frame(base_data_3columns.writing[,2], drop=FALSE, stringsAsFactors=FALSE) #assumes Col. 2 is the ta.commment column

base_data_3columns.technical <- subset(base_data, code.subject == "3. Technical and Scientific", select = colnames.base_data)
base_data_comments.technical <- as.data.frame(base_data_3columns.technical[,2], drop=FALSE, stringsAsFactors=FALSE) #assumes Col. 2 is the ta.commment column

base_data_3columns.logic <- subset(base_data, code.subject == "4. Logic and Thinking", select = colnames.base_data)
base_data_comments.logic <- as.data.frame(base_data_3columns.logic[,2], drop=FALSE, stringsAsFactors=FALSE) #assumes Col. 2 is the ta.commment column

base_data_3columns.praise <- subset(base_data, code.subject == "5. Praise or Concern", select = colnames.base_data)
base_data_comments.praise <- as.data.frame(base_data_3columns.praise[,2], drop=FALSE, stringsAsFactors=FALSE) #assumes Col. 2 is the ta.commment column

base_data_3columns.misconduct <- subset(base_data, code.subject == "6. Misconduct", select = colnames.base_data)
base_data_comments.misconduct <- as.data.frame(base_data_3columns.misconduct[,2], drop=FALSE, stringsAsFactors=FALSE) #assumes Col. 2 is the ta.commment column

base_data_3columns.narrative <- subset(base_data, code.subject == "12. Narrative Comments", select = colnames.base_data)
base_data_comments.narrative <- as.data.frame(base_data_3columns.narrative[,2], drop=FALSE, stringsAsFactors=FALSE) #assumes Col. 2 is the ta.commment column

base_data_3columns.sciname <- subset(base_data, code.subject == "Scientific Name", select =  colnames.base_data)
base_data_comments.sciname <- as.data.frame(base_data_3columns.sciname[,2], drop=FALSE, stringsAsFactors=FALSE) #assumes Col. 2 is the ta.commment column
```

This second series of commands creates tokenized words lists for all comments together, and token subsets for each comments subset.
```{r}
base_data_comments.all.tokens <- base_data_comments %>% 
  unnest_tokens(word,ta.comment)
base_data_comments.basic.tokens <- base_data_comments.basic %>% 
  unnest_tokens(word,ta.comment)
base_data_comments.writing.tokens <- base_data_comments.writing %>% 
  unnest_tokens(word,ta.comment)
base_data_comments.technical.tokens <- base_data_comments.technical %>% 
  unnest_tokens(word,ta.comment)
base_data_comments.logic.tokens <- base_data_comments.logic %>% 
  unnest_tokens(word,ta.comment)
base_data_comments.praise.tokens <- base_data_comments.praise %>% 
  unnest_tokens(word,ta.comment)
base_data_comments.misconduct.tokens <- base_data_comments.misconduct %>% 
  unnest_tokens(word,ta.comment)
base_data_comments.narrative.tokens <- base_data_comments.narrative %>% 
  unnest_tokens(word,ta.comment)
base_data_comments.sciname.tokens <- base_data_comments.sciname %>% 
  unnest_tokens(word,ta.comment)

#Display the last set of tokens to show process finished.
base_data_comments.sciname.tokens
```


***

# Tokenizing Text of Comments
Create a single-column data frame of just the comments for ALL lines. Then check that the data was written to the file as characters not factors.



####If you must convert data to a Tibble:
To make tokenizing work you must convert the desired set of extracted data into a tibble. You HAVE to use a tibble to tokenize text lines. Trying to do it straight from a values table will throw an error.

```
base_data_comments_50_tibble <- tibble(line = seq_along(data), text = base_data_comments_50(c[,2])) #tibble-izer

#base_data_comments_50_tibble %>% 
#  unnest_tokens(word,ta.comment)
```
unique.record, code.subject, code.structure
base_data


***
***

##FWIW
This command uses a tibble with 3 data columns, and successfully unnests the tokens. It is messy because the other columns are duplicated, BUT at least it is possible to wrangle a multi-column tibble for tokenization.
```{r}
Abase_data_3columns.sciname.tokens <- base_data_3columns.sciname %>% 
  unnest_tokens(word,ta.comment)
```



***
#Approach ONE: Work backwards from existing sorting to get criteria.
I have a pre-defined set of traits. Can I build from that?

Do it using naive Bayes. 

##Step 1.1: Regenerate the groups in two-dimensional array


##Step 1.2: Yada


##Step 1.3: Yada


##Step 1.4: Yada


***
#Approach TWO: Assume no a priori groups, and look for natural clusters. 



##Step 2.1: Yada


##Step 2.2: Yada


##Step 2.3: Yada


##Step 2.4: Yada

