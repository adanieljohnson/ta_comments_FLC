---
title: "Round 1 Notebook Fall 2018"
author: "Dan Johnson"
date: "9/12/2018"
output: html_notebook
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Background Overview
We are interested in understanding how students learn to write in biology, and how we can apply insights from data science towards improving that process.

Currently we have an NSF-sponsored project to assess whether scripted instruction and automated feedback can accelerate student growth as writers. We are identifying the most common writing problems that students have by extracting and qualitatively coding comments that instructors write on lab reports. Instructors' comments are categorized by topic (writing issues, technical errors, logical flaws, etc.), structure (copyediting, prescriptive, reflective) and other categories based on specific terms, phrases, or other features within the text of the comment.
 
This analysis is informative but impractical for routine use, given a single semester's dataset contains >11,000 text-based comments plus metadata. Inter-rater reliability and consistency over time also are major concerns. 

The goals for this Faculty Learning Community (FLC) project are to design, evaluate and document an automated procedure for classifying text-based TA comments quickly and reliably. Looking beyond the current project, we hope to use automated text categorization of instructor comments to identify the most frequent writing problems students have **as a group** (not just individually), then adjust writing training objectives so they focus specifically on those issues first.  

####\     

## Initial De-Identification and Data Structure
The baseline dataset is an Excel table containing ~11,000 de-duplicated data rows and 28 data columns. Data structure is tidy, in each row represents ONE unique TA comment on student writing. Metadata and assigned coding is stored in separate columns.

To anonymize the original dataset, a separate file (ID_lookup.xlsx) was created containing identifiable names of TAs and email addresses for students.  Two additional columns of random identifiers were created: 

1.  500 non-repeated unique IDs for TAs in format: **TA_7lL5KRzf**
2.  10,000 non-repeated unique IDs for students in format: **Std_H6n5PxOy**

Individual TAs and student emails were paired with corresponding random IDs, then both TA names and student email addresses replaced using an Excel LOOKUP function. 

The finalized data table imported from the .CSV file is organized as follow:


Col#|Name|Assigned By|Description of Column Data
-----|--------|----------|----------------------------------------------------------
1|unique.record|PI|Unique line identifier for each data line in tables. SemesterYear.##### (example: Sp18.00012 is line 12 in dataset from Spring '18)
2|report.id|Qualtrics|R_XX value
3|sort|PI| (Can drop) Original sorting value. Duplicates of this number indicate a complex comment was split
4|report.title|Mixed|Title of the file from which a comment was extracted. **Data are deleted from anonymized dataset.**
5|student|PI|Each student was assigned an anonymous, unique ID that can be traced back to a WFU-issued email address for the student who wrote report. Format is **"Std_H6n5PxOy"** (this ID is equivalent to "NA").
6|course|PI or TA|Course bins: 1113, 114, 214
7|ta|PI|Each TA or instructor marking student papers was assigned an anonymous, unique ID that can be traced back to that ibstructor. Format is **"TA_7lL5KRzf"** (this ID is equivalent to "NA").
8|lab|PI|Topic of the lab on which report was written
9|tag|Student|First or second of semester. Observed that students often enter incorrectly. Revalidate 
10|type.TA|TA|Submission or revision version. These are unreliable; we found that students often enter incorrectly. Revalidate before analysis
11|grade.TA|TA|Assigned grade from the TA
12|grading.time|TA|Entered by TA, in minutes, or minutes.seconds
13|Rank|PI|Housekeeping data from de-duplication process. 
14|hypothesis.ok|TA|Is a hypothesis present, Yes or No?
15|data.ok|TA|Are data present, Yes or No?
16|citation.ok|TA|Are there citations in introduction and discussion, Yes or No?
17|interpretation.ok|TA|Are the data interpreted, Yes or No?
18|organization.ok|TA|Is report properly organized into 8 sections, Yes or No?
19|techflaws.ok|TA|Is report free of technical errors, Yes or No?
20|writing.ok|TA|Is report free of writing errors, Yes or No?
21|comments.incorporated|TA|If a revision, did student incorporate comments from submission into revision, Yes or No?
22|ta.comment|TA|Text of the actual extracted comment made by the TA.
23|code.subject|PI|Described in next table
24|code.structure|PI|\ ''
25|code.locus|PI|\ ''
26|code.scope|PI|\ ''
27|code.tone|PI|\ ''
28|code.notes|PI|\ ''
####\  

**Coded Characteristics** are encoded using a limited set of choices. Topics and choice vocabulary are described in detail in **codebook_for_comments.md**.  

Name|Description|Status in Sp18 D'set
------------|----------------------------------|-----------------------
code.subject|What is main subject?|Completed
code.structure|How is comment structured? Copy correction, etc.|Completed
code.locus|Where is locus of control? Corrective, directive, reflective?|Partial
code.scope|Can comment carry to other parts of report, beyond report?|Some have been done
code.tone|Is wording likely to be seen as positive, neutral, or negative?|Not done yet
code.notes|Personal notes entered while coding|Not part of active dataset
***
####\   

## Questions I want to ask

1.  Is there a set of characteristics in the text of comments that identifies the **Subject** of the comments with >95% accuracy compared to human readers?
2.  Is there a set of characteristics in the text of comments that identifies the **Structure** of the comments with >95% accuracy compared to human readers?  

####\   

## Preliminary Strategy
I have 2 possible approaches:

1.  Using clustering as unsupervised model to create groups de novo.
2.  Use naive Bayes to test whether specific words or n-grams are strongly linked to specific groups. 

####\   

## Codebook and Procedures
###Code Scheme for TA Comments on Lab Reports
We evaluating TAs’ comments on student reports to see if and how their comments change as students get more feedback from automated system. 

To build an initial data table, all reports containing TAs’ comments from a single semester were downloaded as MS Word documents. Files were processed using BaSH scripts to extract relevant file data (author, ID#, etc.) and individual TA comments from the raw XML, and write it to a single CSV file. TA comments containing multiple points or suggestions were replicated and edited to produce a working data table with only one comment per entry row. 

In the first semester, ~12,000 individual TA comments were extracted from student reports. Comments were categorized based on:

*  Subject. What is main focus of the comment?
*  Structure. Does the comment point out an error only, or provide more information?  If the latter, how much and in what form? Does the comment focus on a fundamental writing issue, or an idiomatic preference of the grader?
*  Locus. Does the comment tell a student what to do, or invite the student to think about a writing issue more deeply?
*  Scope. Does the comment focus on corrections in context of current work only, or connect it to other past or future work by that student? Does it support comparison and transfer?
*  Tone. Is the instructor’s comment affectively neutral (should be most common type), positive (aims to build confidence and self-efficacy), or negative (likely to erode self-efficacy)?

In addition to the qualitative categories above, we coded for:

*  Response. Did student act on the comment?
*  Length. How long is a typical comment?
*  Count. How many comments did the instructor make on average?
*  Errors. Comment was factually incorrect, or mis-stated grading policies, criteria.  

####\  

####Establishing the Inclusion and Exclusion Criteria
I assigned TA comments to categories and sub-categories using a codebook established iteratively using two training sets of 120 and 1200 comments selected randomly from the full dataset. 

Briefly, each main category (Subject, Structure, etc.) was divided into 4-8 loosely defined provisional sub-categories. Then each comment from the 120-item training set was assigned to a provisional sub-category within each category. Sub-categories were not mutually exclusive; for example, if a comment was assigned to the “Writing Quality” sub-category under the “Subject” category, that comment could be assigned to ANY of the provisional sub-categories in “Structure,” ANY of the sub-categories in “Locus,” etc.

When comments could not be assigned with high confidence to a provisional sub-category, our original defining features of the provisional sub-categories were refined further and the previously assigned comments re-evaluated using the updated criteria, until all 120 comments in the training set could be assigned reliably. 

Once stable sub-categories were established, examples of TA comments belonging to each sub-category were extracted from the main dataset and combined with finalized criteria to form a working code scheme. The working code scheme was tested and refined again by categorizing the second training set of 1200 comments, to produce the final coding scheme. 

To score the full data table, the 1200 scored training comments were rejoined with the unscored comments, and all comments alphabetized. This step distributed the pre-scored comments randomly within the larger data table, which helped us identify any comments which had been duplicated during the training cycles.

After incorporating the training set back into the main set, I scored the full set of ~12,000 comments. The randomly re-inserted training comments provided a routine reminder of how the key traits had been applied, which further improved scoring consistency. 

**Still need to do:** Have a second reader independently score another sample of ~1,200 randomly selected comments using the coding scheme. In cases of scoring discrepancies, the first and second reader need to compare and discuss scores, then revise the draft set of key characteristics as needed until inter-rater agreement reaches 0.80 or greater. This will become finalized set of key characteristics used for scoring TA comments in all subsequent semesters and for developing the criteria for automated tagging of comments.  

####\   

###Proof of Concept for Identifying Comments Using Text Features
The goal is to find ways to automate this data processing and analysis. My first approach was to look at word frequencies in sub-groups.

To test whether this is even possible, all TA comments assigned to each category and sub-category were combined in sub-sets, then frequency tables for single words, bi-grams and tri-grams were created for each subset using Laurence Anthony's TagAnt and ConcAnt tools (http://www.laurenceanthony.net/)  
Uninformative high-frequency terms (i.e., articles, conjunctions, etc.) were deleted then the draft lists from the sub-categories were scanned for duplicate or high-frequency terms that might represent "signifier terms" for a particular category. This approach provided severalproof of concept" examples suggesting this approach would be feasible.

####\  

**Example 1:** When I reviewed the word lists I found that the 2-gram "basic criteria" was informative. The 2-gram phrase "basic criteria" is in 78 comments. Of these, 53/78 are Subject: Basic Criteria, 12/78 are Subject: Writing, 9/78 are in Subject: Technical, and 1/78 are in Subject: Logic. 

When I compared these raw frequencies to total number of comments in each subject, I found 53/211 comments in Subject: Basic Criteria contain this 2-gram; 12 of 2578  comments in Subject: Writing have it,  9 of 5409 comments in Subject: Technical have it, and 1 of 1142 in Subject: Logic have it.

When I calculated the fraction of ALL comments in each subject area, I found:

*  25.12% of comments in Subject: Basic Criteria contain this 2-gram
*  0.47% of comments in Subject: Writing contain this 2-gram
*  0.17% of comments in Subject: Technical contain this 2-gram
*  0.09% of comments in Subject: Logic contain this 2-gram.

####\   

**Example 2:** Similarly the word "fail" is in 41 comments. Of these, 25/41 are Subject: Basic Criteria, 6/41 are Subject: Writing, 4/41 are in Subject: Technical, and 4/41 are in Subject: Logic. 

When I calculated the fraction of ALL comments in each subject area, I found:

*  11.85% of comments in Subject: Basic Criteria contain this word
*  0.04% of comments in Subject: Writing contain this word
*  0.07% of comments in Subject: Technical contain this word
*  0.35% of comments in Subject: Logic contain this word.

####\   

**Example 3:** Colloquial 
The term "colloquial" is **unique** to Subject: Writing, and to Structure: Narrative. It is not present in any of the other Subject or Structure groups.   

####\  

**Example 4:** Primary Literature is a less clear 2-gram that I am less sure is useful, but it seems like it should be. These are the individual frequencies.

*  Subject: Basic Criteria. Primary is 17, literature is 18, out of 528 terms
*  Subject: Writing. Literature is 66, primary is 113, out of 2236 terms
*  Subject: Technical. Primary is 145, literature is 160, out of 3120 terms
*  Subject: Logic. Literature is 97, primary is 145, out of 2106 terms
*  Subject: Praise or concern. Literature is 57, primary is 155, out of 413 terms
 
