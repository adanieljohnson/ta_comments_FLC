---
title: "Log Odds for Word Usage Method"
author: "Dan Johnson"
date: "11/24/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##Preparing and Pre-Checking the Initial Dataset

Call in tidyverse, other libraries.
```{r Libraries}
library(tidyverse)
library(tidytext)
library(dplyr)
library(tidyr)
library(ggplot2)
library(scales)
```

CSV file named "base_data.csv" already has correctly organized text and meta-data. First block reads in CSV, then filters and pulls rows and columns to a new working file.

```{r}
# Read CSV.
base_data <- read_csv(file='data/coded_full_comments_dataset_Spring18.csv')

# Isolate table rows to compare.
frequency_writing <- filter(base_data, code.subject=="2. Writing Quality"|code.subject=="3. Technical and Scientific"|code.subject=="4. Logic and Thinking")

#Reduce larger dataframe to just 2 required columns of data  
frequency_writing.subcolumns <- frequency_writing %>% select(1, 22:23)
```

Next block tokenizes the TA comments as individual words and removes stop words.

```{r}
# Tokenize phrases, remove stop words listed in standard reference file.
base_data_tokenized <- frequency_writing.subcolumns %>% 
  unnest_tokens(word,ta.comment) %>% 
  anti_join(stop_words)
```

At this point the tibble has "Unique.Record" in column 1, "code.subject" in column 2, and the unnested "word" in column 3. All columns are class = "character". Stop words from the R "stop_words" data file have been removed.

##Run 1: Comparing Techncial and Logic Word Frequencies Against Writing Quality

```{r}
#Create groups by code.subject, calculates proportional frequency in each group.
#Final step creates 5 columns: code.subject, n, word, total, frequency.
base_data_tokenized_sorted2 <- base_data_tokenized %>% 
  group_by(code.subject) %>%
  count(code.subject, word, sort=TRUE) %>% 
  left_join(base_data_tokenized %>% 
  group_by(code.subject) %>% 
  summarise(total = n())) %>%
  mutate(freq = n/total)
```


###Compare Pearson Correlations of Word Frequencies
If the words used by TAs in their comments differ between the categories, there should be fairly low correlations.

Re-organize the data for comparisons. This pattern sets 2. Writing Quality as the first dataset, then all other data in remaining columns.
```{r}
#Groups by code.subject, calculated proportional frequency in each group.
#Final step creates 4 columns: word, 2. Writing Quality, code.subject, proportion
#Stats need this format to be able to compare 2. WQ against other values.
base_data_tokenized_sorted2 <- base_data_tokenized %>% 
  group_by(code.subject) %>%
  count(code.subject, word, sort=TRUE) %>% 
  mutate(proportion = n / sum(n)) %>% 
  select(-n) %>% 
  spread(code.subject, proportion) %>% 
  gather(code.subject, proportion, `3. Technical and Scientific`:`4. Logic and Thinking`)
```

Tibble called "base_data_tokenized_sorted2" is the final 4-column dataframe for statistical analysis. Individual words are in column 1, "2. Writing Quality" frequency is in Column 2, "code.subject" is in column 3, and "proportion" is in column 4. Columns 1 and 3 are class = "character", 2 and 4 are numeric ranges. 

This structure is needed in order to compare word (in Column 1) frequencies for 2. Writing Quality (Column 2) as Y values against the frequencies of those same words (in Column 4) as X axes. WHICH subset of X values to use is coded in Column 3.

The steps below determine Pearson correlation between the frequency of words in Writing Quality versus the other two groups.

```{r}
cor.test(data = base_data_tokenized_sorted2[base_data_tokenized_sorted2$code.subject == "3. Technical and Scientific",], 
         ~ proportion + `2. Writing Quality`)
```

```{r}
cor.test(data = base_data_tokenized_sorted2[base_data_tokenized_sorted2$code.subject == "4. Logic and Thinking",], 
         ~ proportion + `2. Writing Quality`)
```



##Run 2: Comparing Writing and Logic Word Frequencies Against Technical and Scientific

```{r}
#Create groups by code.subject, calculates proportional frequency in each group.
#Final step creates 5 columns: code.subject, n, word, total, frequency.
base_data_tokenized_sorted3 <- base_data_tokenized %>% 
  group_by(code.subject) %>%
  count(code.subject, word, sort=TRUE) %>% 
  left_join(base_data_tokenized %>% 
              group_by(code.subject) %>% 
              summarise(total = n())) %>%
  mutate(freq = n/total)
```


###Compare Pearson Correlations of Word Frequencies
If the words used by TAs in their comments differ between the categories, there should be fairly low correlations.

Re-organize the data for comparisons. This pattern sets 3. Technical and Scientific as the first dataset, then all other data in remaining columns.
```{r}
#Groups by code.subject, calculated proportional frequency in each group.
#Final step creates 4 columns: word, 3. Technical and Scientific, code.subject, proportion
#Stats need this format to be able to compare 3. TS against other values.
base_data_tokenized_sorted3 <- base_data_tokenized %>% 
  group_by(code.subject) %>%
  count(code.subject, word, sort=TRUE) %>% 
  mutate(proportion = n / sum(n)) %>% 
  select(-n) %>% 
  spread(code.subject, proportion) %>% 
  gather(code.subject, proportion, `3. Technical and Scientific`:`4. Logic and Thinking`)

# THERE IS AN ERROR HERE I CANNOT DEBUG! If I change "3. Technical and Scientific"  to "2. Writing Quality", the structure of the tibble breaks. It goes from the correctly sorted 4 columns to incorrectly sorted 3 columns. It is NOT A TYPO; I cannot use a different order, or any other format. 

```



Tibble called "base_data_tokenized_sorted3" is the final 4-column dataframe for statistical analysis. Individual words are in column 1, "3. Technical and Scientific" frequency is in Column 2, "code.subject" is in column 3, and "proportion" is in column 4. Columns 1 and 3 are class = "character", 2 and 4 are numeric ranges. 

This structure is needed in order to compare word (in Column 1) frequencies for 3. Technical and Scientific (Column 2) as Y values against the frequencies of those same words (in Column 4) as X axes. WHICH subset of X values to use is coded in Column 3.

The steps below determine Pearson correlation between the frequency of words in Technical and Scientific versus the other two groups.

```{r}
cor.test(data = base_data_tokenized_sorted3[base_data_tokenized_sorted3$code.subject == "3. Technical and Scientific",], 
         ~ proportion + `2. Writing Quality`)
```

```{r}
cor.test(data = base_data_tokenized_sorted3[base_data_tokenized_sorted3$code.subject == "4. Logic and Thinking",], 
         ~ proportion + `2. Writing Quality`)
```







###Visual Comparison of Word Distributions 

If the words used by TAs in comments are not different between the categories, the slopes of the lines in the three comparison graphs should be close to 1, with intercepts approaching zero. If there are differences in usage, the plots should not intercept at (0,0), and there should be an offset of word frequencies from X, Y baselines.

```{r}
frequency2 <- base_data_tokenized_sorted2 %>% 
  select(code.subject, word, proportion) %>% 
  spread(code.subject, proportion) %>%
  arrange(`3. Technical and Scientific`,`4. Logic and Thinking`)
```

```{r}
ggplot(frequency2, aes(`3. Technical and Scientific`,`4. Logic and Thinking`)) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.25, height = 0.25) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  geom_abline(color = "red")
```

```{r}
ggplot(frequency2, aes(`2. Writing Quality`,`4. Logic and Thinking`)) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.25, height = 0.25) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  geom_abline(color = "red")
```

```{r}
ggplot(frequency2, aes(`2. Writing Quality`,`3. Technical and Scientific`)) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.25, height = 0.25) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  geom_abline(color = "red")
```





