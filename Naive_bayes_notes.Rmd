---
title: "Naive Bayes"
author: "Dan Johnson"
date: "12/16/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Notes on Naive Bayes Method

The objective of this project is to classify SMS messages as spam or ham (not spam). A Naive Bayes classifier approach will be used. This example is taken from chapter 4 of Machine Learning with R, Second Edition"


An example of the conditional probability that will be computed is as follows: 

P(Spam|Hospital) = P(Hospital|Spam)P(Spam)/P(Hospital) 

which is the formula for determining the probability that a message is spam given that it contains the word "Hospital" in the message.


##Loading Data Into R.

```{r}
sms_raw <- read.table("~/Dropbox/Coding_Tools/R_Environment/R_Projects/ta_comments_FLC/data/Bayes/SMSSpamCollection.txt", header = FALSE, sep = "\t", quote = "", col.names = c("type","text"), stringsAsFactors = FALSE)
```

type = code.subject;  text = ta.comment (2. Writing Quality; 3. Technical and Scientific, 4. Logic and Thinking)

```{r}
names(sms_raw)[1] <- "type"
names(sms_raw)[2] <- "text"
```


### The data frame has 5574 observations with 2 observations of either spam or ham.

```{r}
str(sms_raw)
```


### As seen above, the "type"" element is a character.Change it to a factor for the analysis.

```{r}
sms_raw2$type <- factor(sms_raw2$type)
str(sms_raw2$type)
table(sms_raw2$type)
```


### Now the "type" variable is a factor with 2 levels. Of the 5574 messages, 747 are spam, which is about 13.4%.

# 2. Text Mining

### For Naive Bayes to run effectively, the test data needs to be transformed. This begins with using the "tm" package in R to create a volitile coprus that contains the "text" vector from our data frame.

```{r}
install.packages("tm")
library(tm)
```

```{r}
sms_corpus2 <- VCorpus(VectorSource(sms_raw2$text))
print(sms_corpus2)
```

### Check out the first few messages in the new corpus, which is basically a list that can be manipulated with list operations.

```{r}
inspect(sms_corpus2[1:3])
```

### Use "as.character" function to see what a message looks like.

```{r}
as.character(sms_corpus2[[3]])
```

### In order to standardize the messages, the data set must be tranformed to all lower case letters. The words "Free", "free", and "FREE" should all be treated as the same word. Use the "tm_map"" funtion in R, and use the "content_transformer" function to transform the text.


```{r}
sms_corpus_clean2 <- tm_map(sms_corpus2, content_transformer(tolower))
```


### Look at third message again to see if our data was transformed.

```{r}
as.character(sms_corpus2[[3]])

as.character((sms_corpus_clean2[[3]]))
```

### The message was indeed tranformed to all lowercase letters. Now to remove the numbers using the "removeNumbers" function.


```{r}
sms_corpus_clean2 <- tm_map(sms_corpus_clean2, removeNumbers)
```


### Remome words that appear often but don't contribute to our objective. These words include "to", "and", "but" and "or".


```{r}
sms_corpus_clean2 <- tm_map(sms_corpus_clean2, removeWords, stopwords())
```

### Remove punctuation as well using the "removePunctuation" function.

```{r}
sms_corpus_clean2 <- tm_map(sms_corpus_clean2, removePunctuation)

as.character((sms_corpus_clean2[[3]]))
```

### Perform "stemming" to the text data to strip the suffix from words like "jumping", so the words "jumping" "jumps" and "jumped" are all transformed into "jump". Stemming can be perfromed using the "tm" package with help from the "SnowballC" package.

```{r}
install.packages("SnowballC")
```

```{r}
library(SnowballC)

sms_corpus_clean2 <- tm_map(sms_corpus_clean2, stemDocument)
```

And now the final step in text mining is to remove white space from the document. 

```{r}
sms_corpus_clean2 <- tm_map(sms_corpus_clean2, stripWhitespace)
```

```{r}
as.character(sms_corpus_clean2[[3]])
```


### Perform tokenization using the "DocumentTermMatrix" function. This creates a matrix  in which the rows indicat documents (SMS messages in this case) and the columns indicate words. It should be noted that the "DocumentTermMaxtrix" function has the power to do all of the text mining above in one command.


```{r}
sms_dtm2 <- DocumentTermMatrix(sms_corpus_clean2)
```


# 3. Data Preparation

### Split our data into training and testing sets, so that after Naive Bayes spam filter algorithm is built it can be applied to unseen data. Divide our data set into 75% training and 25% testing.

```{r}
.75 * 9129
.25 * 9129
```

### Because the dataset is random, the first 4180 messages can be used for the training set - there's no need to randomize the data first.

```{r}
sms_dtm_train2 <- sms_dtm2[1:6846, ]
sms_dtm_test2 <- sms_dtm2[6847:9129, ]
```

### Save vectors labeling rows in the training and testing vectors

```{r}
sms_train_labels2 <- sms_raw2[1:6846, ]$type
sms_test_labels2 <- sms_raw2[6847:9129,]$type
```

### Make sure that the proportion of spam is similar in the training and testing data set.

```{r}
prop.table(table(sms_train_labels2))

prop.table(table(sms_test_labels2))
```

### Each have approx. 13% spam.

# 4. Visualization

### Create a wordcloud of the frequency of the words in the dataset using the package "wordcloud".

```{r}
install.packages("wordcloud")
```
```{r}
install.packages("RColorBrewer")
```


```{r}
library(wordcloud)
wordcloud(sms_corpus_clean2, max.words = 50, random.order = FALSE)
```


### Compare wordclouds between spam and ham.

```{r}
TWO <- subset(sms_raw2, type == "2. Writing Quality")
THREE <- subset(sms_raw2, type == "3. Technical and Scientific")
FOUR <- subset(sms_raw2, type == "4. Logic and Thinking")

wordcloud(TWO$text, max.words = 50, scale = c(3, 0.5))
wordcloud(THREE$text, max.words = 50, scale = c(3, 0.5))
wordcloud(FOUR$text, max.words = 50, scale = c(3, 0.5))
```


# 5. Preparation for Naive Bayes

### Remove words from the matrix that appear less than 5 times.

```{r}
sms_freq_words <- findFreqTerms(sms_dtm_train, 5)
str(sms_freq_words)
```


### Limit our Document Term Matrix to only include words in the sms_freq_vector. We want all the rows, but we want to limit the column to these words in the frequency vector.

```{r}
sms_dtm_freq_train <- sms_dtm_train[ , sms_freq_words]
sms_dtm_freq_test <- sms_dtm_test[ , sms_freq_words]
```


### The naive bayes classifier works with categorical reatures, so we need to convert the matrix to "yes" and "no" categorical variables. To do this we'll build a convert_counts function and apply it to our data.


```{r}
convert_counts <- function(x) {
  x <- ifelse(x > 0, "Yes", "No")
}
```


### This replaces values greater than 0 with yes, and values not greater than 0 with no. Let's apply it to our data.

```{r}
sms_train <- apply(sms_dtm_freq_train, MARGIN = 2, convert_counts)

sms_test <- apply(sms_dtm_freq_test, MARGIN = 2, convert_counts)
```


### The resulting matrixes will be character type, with cells indicating "yes" or "no" if the word represented by the column appears in the message represented by the row.



# 6. Train Model on the Data.


### Use the e1071 package to impliment the Naive Bayes algorithm on the data, and predict whether a message is likely to be spam or ham.

```{r}
install.packages("e1071")
library(e1071)
```



```{r}
sms_classifier <- naiveBayes(sms_train, sms_train_labels)

```

# 7. Predict and Evaluate the Model.

```{r}
sms_test_pred <- predict(sms_classifier, sms_test)
```


### Evaluate the predition with the actual data using a crosstable from the gmodels package.

```{r}
library(gmodels)
CrossTable(sms_test_pred, sms_test_labels, prop.chisq = FALSE, prop.t = FALSE, dnn = c('predicted', 'actual'))
```



```{r}
29/1379
```

### As shown in the table only 29/1379 messages were classified incorrectly. This means that the algorithm classifed the testing set as spam or ham with approx. 98% accuracy. That's impressive. To improve the model, one might tamper with the Laplace value, colect more sms data, or try splitting the dataset randomly into training and testing.

### I suspect the accuracy would increase as the dataset gets bigger. The more data there is to train the algorith, the more effective it would be in predicting Spam or Ham.



### Show the 5 most frequent words in the sms data:

```{r}
sack <- TermDocumentMatrix(sms_corpus_clean)
pack <- as.matrix(sack)
snack <- sort(rowSums(pack), decreasing = TRUE)
hack <- data.frame(word = names(snack), freq=snack)
head(hack, 5)
```

### And the 5 most frequent words from each class:

```{r}
wordcloud(spam$text, max.words = 5, scale = c(3, 0.5))
wordcloud(ham$text, max.words = 5, scale = c(3, 0.5))
```

### As shown in the word clouds, the most frequent words from the spam messages are "call", "free", "now", "mobile", and "text". And in the Ham messages, the 5 most frequent words are "can", "get", "just", "will", and "now".


