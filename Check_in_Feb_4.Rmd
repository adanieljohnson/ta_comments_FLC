---
title: "Feb 4 Check-In"
author: "Dan Johnson"
date: "2/3/19"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#New ideas From FLC Meeting #6

Need help with:

Coding issues

*  How to ID, prune unneeded packages. NO SIMPLE SOLUTION. Cut and test seems best.

*  When I join columns in correlation analysis, if a word is MISSING from **any subset** the data for that word is pruned from the full dataset. I am losing the unique words this way. 
    +  Assume the NA would blow up correlations, which is why it is done this way.
    +  Need to figure out how to join without pruning, then find the UNIQUE words.

Post-Processing issues. Use broom package

*  How to calculate the value automatically in the Naive Bayes final step. Data post-processing. I have stable code for my binomial naive Bayes classifier, and I am screening different permutations to see what the best approach is to refining the model. There is one step I cannot figure out how to code though.My primary output is a cross-comparison table showing what fraction of the test set comments is correctly identified. Adding the counts diagonally and dividing by the total number of test cases gives me the overall accuracy.The table has all of the data that I want to use for other calculations, but I cannot figure out how to either: 1) call values from the table, or 2) pull 

 Output issues
 
*  How to take the Excel/csv tables I have for permutations of naive Bayes, Pearson's r, and bring them in for display in summary page. broom


Add to My Code:

*  shell(cmd, shell, flag = "/c", intern = FALSE, wait = TRUE,
      translate = FALSE, mustWork = FALSE, ...) runs UNIX commands
https://www.rdocumentation.org/packages/base/versions/3.4.0/topics/shell

*  Using csvy format. R supports .csvy using csvy package and read_csvy() and write_csvy()
Reference Sites:

*  Using broom for cleanup
https://cran.r-project.org/web/packages/broom/vignettes/broom.html

Packages to Explore:

*  fs. https://fs.r-lib.org/index.html

fs provides a cross-platform, uniform interface to file system operations. It shares the same back-end component as nodejs, the libuv C library, which brings the benefit of extensive real-world use and rigorous cross-platform testing. fs functions are divided into four main categories:

    path_ for manipulating and constructing paths
    file_ for files
    dir_ for directories
    link_ for links
fs is designed to work well with the pipe, though because it is a minimal-dependency infrastructure package it doesnâ€™t provide the pipe itself. You will need to attach magrittr or similar. fs functions also work well in conjunction with other tidyverse packages, like dplyr and purrr.

To Do List:

*  Best practices when writing R code
https://swcarpentry.github.io/r-novice-inflammation/06-best-practices-R/

#File Management

```{r}
headlines2 <- read_docx("R_0eqZDDQ0nG2obuh_text.docx") %>% head(9)
View(headlines2)
```

R can manipulate files and folders. This is a primer:
http://theautomatic.net/2018/07/11/manipulate-files-r/

Get, change working directory
Create files of any type based on extension
Copy a file or folder
List files in a directory, including by sub-type
Use rowbind to join file contents
Get snapshot data
Remove files, folders
Check for files, folders
Get the basenames for files, folders
Get file extensions
Call shell to open the file
Move a file




# read in all the CSV files
all_data_frames <- lapply(list.files(pattern = ".csv"), read.csv)
 
# stack all data frames together
single_data_frame <- Reduce(rbind, all_data_frames)


Default
draft
General_workflow
initial_exploration
Naive_bayes_V2b, r, yn
pre_work
project
questions
reference_tables
course_notes

ta_comments_FLC
Text Classifier Models
Random_forest_notes
Latent_dirichlet_allocation_notes
  folder=data
    codebook_for_comments.md
    code_column_names.md
    TA_and_Student_IDs.csv
    ID_lookup.csv
  folder=docs_and_notes
    Grading Criteria.rmd
    Raw_data_structure.rmd
  


###Planned Project Pages
It may seem excessive, but my thinking is that:
1. Our group is the first so should establish some guidelines for creating tools.
2. Work I put in here translates over to the final, shared product for NSF project.

*  toc = Table of Contents
*	 project = Project Summary

*	 Background
    +  Theoretical background on text classifiers
    +  Practical background on project challenges

*	 Project Methods and Guides
    +  Anonymizing Data
    +  Standard Workflow
    +  Reference Tables
        -  Recoding lists
    +  Project Codebook

*  Pages template:
    +  Page Goals
    +  Page Overview
    +  Page Content

*  Initial Exploration
    +  Data Processing
    +  Preliminary Visualization
		

*  Deeper Analyses
    +  Categorical Naive Bayes
        -  Baseline cNB
        -  Randomized cNB
        -  Binary cNB
        -  Summary of cNB Findings

  
***
***

#Jan 14 Check-In, and New ideas From FLC Meeting #5

Check my data for correlations.
	"cor" blows up on missing data
	Try "use" pairwise complete


*  Use R profile for storing API tokens

```{r}
file.edit(file.path("~", ".Rprofile")) # edit .Rprofile in HOME
file.edit(".Rprofile") # edit project specific .Rprofile    

# There is information about the options you can set via
# help("Rprofile")
```

Tools to consider:

*  UDPipe for syntactic parsing
*  Stylometry library
*  QualtRics package for connecting to API (no longer supported by CRAN)
*  By hand exploration followed up with computational exploration

Reference sources:

*  R Open Sci


To Do List:

*  Need to add some data intermediates to my work to help others visualize.
*  Taskviews on CRAN
*  Look at lemmatization, not stemming


Look into Bayes model commands:

*  prob
*  predict

Packages to evaluate

*  caret
*  skimR
*  parsnip
*  recipe
*  tidymodels
*  UDPipe
*  package = usethis




##What Is Working



Suggested: 
Check my data for small errors that break correlations.
	"cor" blows up on missing data
	Try "use" pairwise complete


Tokenizer and terms cleanup seems to work well. (Still struggling with understanding how data get transformed in certain copied code blocks.)
  Suggested: Look into lemmatization, not stemming



## Coding & Interpretation Questions

1.  Randomization. Does the straight split I used bias the data? 
  Comment: YES, so instead use set.seed to pick more randomized dataset.


Suggestions:
  Try: 
    Leave one out cross-validation
    K-folds cross-validation


I do have a few things for you to consider before we do get together. The first thing I suggest you do, which shouldn't be too much of a problem to create, is to provide the output of some of your datasets (particularly thinking about the contrast sets that you use in the correlation analysis). A quick `glimpse(data)` printed out would help me get a better sense of the structure you are using as input and output at each stage of your analysis.   



Third thing that comes to mind has to do with the Naive Bayes analysis. As you've seen, naive bayes function from the `e1071` package only works with categorical features. For many NLP tasks this oversimplifies features making the appearance of a word a 'yes' or 'no' decision, which reduces information concerning the frequency of occurrence of words. It's been a while since I've used naive bayes in my own work, but last time I did, I ended up writing my own algorithm to deal with non-categorical features. I'm not altogether happy with the algorithm I coded, but when we meet we can talk about whether there are other package options out there now, or whether my function might be of some help to you.

Anyway, things are looking good. Keep up the good work and great documentation!




## Bigger Questions
Python/Scython porting

After posting an initial naive Bayes trial, Jerid wrote:
>Naive bayes function from `e1071` package only works with categorical features. For many NLP tasks this oversimplifies features making the appearance of a word a 'yes' or 'no' decision.

I spent time looking at various methods as part of background research. The summary is posted elsewhere. Question is whether I should focus on:

*  Continued refinement of NB, or exploring other supervised models? 
    +  How deep to go into different methods?
    +  When to drop one model, move to another?
*  Using my existing code-book to reverse engineer a pure rules based method instead?
    +  Faster for me, but what about others?
*  Extending previous item, create a HYBRID of statistical and rules-based?
    +  Words to evaluate are pre-selected limited vocabulary (see codebook for lists)
    +  Words are scored present/absent in a comment, then comment sorted based on most likely.
*  Step back and rethink the categories using LDA, etc.? 
    +  Seems counter-productive given I have a model built already.

 
